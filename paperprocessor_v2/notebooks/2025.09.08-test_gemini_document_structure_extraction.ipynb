{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini Document Structure Extraction\n",
    "\n",
    "This notebook uses OpenRouter with Gemini models to extract document structure from research papers:\n",
    "1. **Gemini-2.0-flash-exp**: Analyzes first 20 pages to understand document structure (element types, levels, recognition patterns)\n",
    "2. **Gemini-2.0-flash-exp**: Extracts actual headers from each page using the structure understanding\n",
    "\n",
    "References: [OpenRouter](https://openrouter.ai/), [Gemini Models](https://ai.google.dev/gemini-api)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Setup\n",
    "Configure OpenRouter client, paths, and constants for document processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setup OpenRouter client and define constants for Gemini document processing.\n",
    "- Keep code simple and explicit; raise if required variables are missing.\n",
    "\"\"\"\n",
    "\n",
    "### CONSTANTS ###\n",
    "from pathlib import Path\n",
    "PDF_PATH: Path = Path(\"/Users/Focus/Downloads/2212.14024v2.pdf\")\n",
    "STRUCTURE_MODEL: str = \"google/gemini-2.5-pro\"  # For document structure analysis\n",
    "EXTRACTION_MODEL: str = \"google/gemini-2.5-flash\"  # For header extraction per page\n",
    "MAX_STRUCTURE_PAGES: int = 20  # Analyze first 20 pages for structure understanding\n",
    "\n",
    "### DEPENDENCIES ###\n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "from typing import Dict, List, Any\n",
    "from dotenv import load_dotenv\n",
    "import openai  # OpenRouter uses OpenAI-compatible API\n",
    "from pydantic import BaseModel, Field\n",
    "import fitz  # PyMuPDF for PDF processing\n",
    "\n",
    "### SETUP CLIENT ###\n",
    "load_dotenv()\n",
    "openrouter_key = os.environ.get(\"OPENROUTER_API_KEY\")\n",
    "if not openrouter_key:\n",
    "    raise RuntimeError(\"OPENROUTER_API_KEY is not set in environment.\")\n",
    "\n",
    "# Configure OpenAI client to use OpenRouter\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=openrouter_key,\n",
    ")\n",
    "\n",
    "if not PDF_PATH.exists():\n",
    "    raise FileNotFoundError(f\"PDF not found: {PDF_PATH}\")\n",
    "\n",
    "print(\"Ready. Structure model:\", STRUCTURE_MODEL)\n",
    "print(\"Extraction model:\", EXTRACTION_MODEL)\n",
    "print(\"PDF:\", PDF_PATH)\n",
    "print(\"Max pages for structure analysis:\", MAX_STRUCTURE_PAGES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) PDF Processing Helpers\n",
    "Helper functions to extract pages from PDF as images for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper functions for PDF processing and image conversion.\n",
    "\"\"\"\n",
    "\n",
    "### HELPER FUNCTIONS ###\n",
    "def extract_pdf_pages_as_images(pdf_path: Path, max_pages: int = None) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract PDF pages as base64-encoded images.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        max_pages: Maximum number of pages to extract (None for all)\n",
    "        \n",
    "    Returns:\n",
    "        List of base64-encoded PNG images\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    \n",
    "    total_pages = len(doc)\n",
    "    pages_to_process = min(max_pages or total_pages, total_pages)\n",
    "    \n",
    "    print(f\"Extracting {pages_to_process} pages from PDF...\")\n",
    "    \n",
    "    for page_num in range(pages_to_process):\n",
    "        page = doc[page_num]\n",
    "        # Convert page to image (higher DPI for better text recognition)\n",
    "        pix = page.get_pixmap(matrix=fitz.Matrix(2.0, 2.0))  # 2x scaling for clarity\n",
    "        img_data = pix.tobytes(\"png\")\n",
    "        \n",
    "        # Convert to base64\n",
    "        img_b64 = base64.b64encode(img_data).decode(\"utf-8\")\n",
    "        images.append(img_b64)\n",
    "        \n",
    "        if (page_num + 1) % 5 == 0:  # Progress update every 5 pages\n",
    "            print(f\"  Processed {page_num + 1}/{pages_to_process} pages\")\n",
    "    \n",
    "    doc.close()\n",
    "    print(f\"Extracted {len(images)} page images\")\n",
    "    return images\n",
    "\n",
    "def create_vision_messages(images: List[str], system_prompt: str, user_prompt: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create message format for vision API with multiple images.\n",
    "    \n",
    "    Args:\n",
    "        images: List of base64-encoded images\n",
    "        system_prompt: System instruction\n",
    "        user_prompt: User instruction\n",
    "        \n",
    "    Returns:\n",
    "        Messages formatted for OpenAI-compatible vision API\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt}\n",
    "    ]\n",
    "    \n",
    "    # Create user message with text and images\n",
    "    content = [{\"type\": \"text\", \"text\": user_prompt}]\n",
    "    \n",
    "    # Add each image\n",
    "    for i, img_b64 in enumerate(images):\n",
    "        content.append({\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{img_b64}\"\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": content})\n",
    "    return messages\n",
    "\n",
    "# Extract all pages as images\n",
    "all_page_images = extract_pdf_pages_as_images(PDF_PATH)\n",
    "structure_images = all_page_images[:MAX_STRUCTURE_PAGES]  # First 20 pages for structure\n",
    "\n",
    "print(f\"Total pages: {len(all_page_images)}\")\n",
    "print(f\"Pages for structure analysis: {len(structure_images)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Document Structure Analysis\n",
    "Use Gemini-2.0-flash-exp to analyze the first 20 pages and understand the document structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Analyze document structure using Gemini-2.0-flash-exp.\n",
    "Extract element types, levels, recognition patterns, and examples.\n",
    "\"\"\"\n",
    "\n",
    "### SCHEMA ###\n",
    "class DocumentElement(BaseModel):\n",
    "    element_type: str = Field(..., description=\"Type of document element (e.g., title, section_header, subsection_header, figure_caption)\")\n",
    "    level: int = Field(..., description=\"Hierarchical level (1=highest, 6=lowest)\")\n",
    "    recognition_pattern: str = Field(..., description=\"How to identify this element (font size, style, formatting, position)\")\n",
    "    examples: List[str] = Field(..., description=\"2-3 actual examples from the document\")\n",
    "\n",
    "class DocumentStructure(BaseModel):\n",
    "    document_type: str = Field(..., description=\"Type of research paper (e.g., conference paper, journal article, preprint)\")\n",
    "    elements: List[DocumentElement] = Field(..., description=\"All document structure elements found\")\n",
    "    notes: str = Field(..., description=\"Additional observations about the document structure\")\n",
    "\n",
    "### PROMPTS ###\n",
    "STRUCTURE_SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert in analyzing research paper document structure. You will receive the first 20 pages of a research paper and need to identify all structural elements.\n",
    "\n",
    "Focus on identifying:\n",
    "1. Hierarchical heading levels (title, section headers, subsection headers, etc.)\n",
    "2. Visual recognition patterns (font sizes, styles, formatting, positioning)\n",
    "3. Specific examples from the document\n",
    "4. Any unique structural patterns in this paper\n",
    "\n",
    "Be precise and detailed in your analysis. This structure understanding will be used to extract headers from all pages.\n",
    "\"\"\"\n",
    "\n",
    "STRUCTURE_USER_PROMPT = \"\"\"\n",
    "Please analyze the structure of this research paper. I need you to identify:\n",
    "\n",
    "1. **Document type**: What kind of research paper is this?\n",
    "2. **All structural elements**: Every type of heading/title/caption you can see\n",
    "3. **Hierarchy levels**: How are elements organized (level 1, 2, 3, etc.)\n",
    "4. **Recognition patterns**: How can each element be identified? (font size, bold/italic, positioning, numbering, etc.)\n",
    "5. **Concrete examples**: 2-3 actual text examples for each element type\n",
    "\n",
    "Focus on headers, titles, section names, subsection names, and any other structural text elements that organize the content.\n",
    "\n",
    "Return EXACTLY this JSON structure (no extra text, no markdown formatting, just JSON):\n",
    "\n",
    "{\n",
    "  \"document_type\": \"string describing type of research paper\",\n",
    "  \"elements\": [\n",
    "    {\n",
    "      \"element_type\": \"main_title\",\n",
    "      \"level\": 1,\n",
    "      \"recognition_pattern\": \"description of how to identify this element\",\n",
    "      \"examples\": [\"example 1\", \"example 2\"]\n",
    "    },\n",
    "    {\n",
    "      \"element_type\": \"section_header\",\n",
    "      \"level\": 2,\n",
    "      \"recognition_pattern\": \"description of how to identify this element\",\n",
    "      \"examples\": [\"example 1\", \"example 2\"]\n",
    "    }\n",
    "  ],\n",
    "  \"notes\": \"additional observations about document structure\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "### PROCESSING ###\n",
    "print(\"Analyzing document structure\")\n",
    "\n",
    "# Create messages for structure analysis\n",
    "structure_messages = create_vision_messages(\n",
    "    images=structure_images,\n",
    "    system_prompt=STRUCTURE_SYSTEM_PROMPT,\n",
    "    user_prompt=STRUCTURE_USER_PROMPT\n",
    ")\n",
    "\n",
    "# Call Gemini-2.0-flash-exp for structure analysis\n",
    "structure_response = client.chat.completions.create(\n",
    "    model=STRUCTURE_MODEL,\n",
    "    messages=structure_messages,\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    max_tokens=4000,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Parse structure response with error handling\n",
    "structure_content = structure_response.choices[0].message.content\n",
    "print(f\"Raw response length: {len(structure_content)} characters\")\n",
    "\n",
    "# Try to parse JSON with better error handling\n",
    "try:\n",
    "    structure_data = json.loads(structure_content)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"JSON parsing failed at position {e.pos}: {e.msg}\")\n",
    "    print(f\"Content around error: '{structure_content[max(0, e.pos-50):e.pos+50]}'\")\n",
    "    \n",
    "    # Try to extract JSON from the response if it's wrapped in other text\n",
    "    import re\n",
    "    json_match = re.search(r'\\{.*\\}', structure_content, re.DOTALL)\n",
    "    if json_match:\n",
    "        json_content = json_match.group(0)\n",
    "        print(\"Attempting to parse extracted JSON...\")\n",
    "        try:\n",
    "            structure_data = json.loads(json_content)\n",
    "            print(\"Successfully parsed extracted JSON!\")\n",
    "        except json.JSONDecodeError as e2:\n",
    "            print(f\"Extracted JSON also failed: {e2.msg}\")\n",
    "            print(\"Creating fallback structure...\")\n",
    "            structure_data = {\n",
    "                \"document_type\": \"research_paper\",\n",
    "                \"elements\": [\n",
    "                    {\n",
    "                        \"element_type\": \"main_title\",\n",
    "                        \"level\": 1,\n",
    "                        \"recognition_pattern\": \"Large bold text at top of first page\",\n",
    "                        \"examples\": [\"Document title\"]\n",
    "                    },\n",
    "                    {\n",
    "                        \"element_type\": \"section_header\", \n",
    "                        \"level\": 2,\n",
    "                        \"recognition_pattern\": \"Bold text with numbering (1., 2., etc.)\",\n",
    "                        \"examples\": [\"1. Introduction\", \"2. Methods\"]\n",
    "                    },\n",
    "                    {\n",
    "                        \"element_type\": \"subsection_header\",\n",
    "                        \"level\": 3, \n",
    "                        \"recognition_pattern\": \"Bold text with sub-numbering (1.1, 1.2, etc.)\",\n",
    "                        \"examples\": [\"1.1 Background\", \"2.1 Dataset\"]\n",
    "                    }\n",
    "                ],\n",
    "                \"notes\": \"Fallback structure due to JSON parsing error\"\n",
    "            }\n",
    "    else:\n",
    "        print(\"No JSON found in response. Using fallback structure.\")\n",
    "        structure_data = {\n",
    "            \"document_type\": \"research_paper\",\n",
    "            \"elements\": [\n",
    "                {\n",
    "                    \"element_type\": \"main_title\",\n",
    "                    \"level\": 1,\n",
    "                    \"recognition_pattern\": \"Large bold text at top of first page\",\n",
    "                    \"examples\": [\"Document title\"]\n",
    "                },\n",
    "                {\n",
    "                    \"element_type\": \"section_header\",\n",
    "                    \"level\": 2,\n",
    "                    \"recognition_pattern\": \"Bold text with numbering (1., 2., etc.)\",\n",
    "                    \"examples\": [\"1. Introduction\", \"2. Methods\"]\n",
    "                }\n",
    "            ],\n",
    "            \"notes\": \"Fallback structure due to JSON parsing error\"\n",
    "        }\n",
    "\n",
    "print(\"\\n=== DOCUMENT STRUCTURE ANALYSIS ===\")\n",
    "print(f\"Document type: {structure_data.get('document_type', 'Unknown')}\")\n",
    "print(f\"Notes: {structure_data.get('notes', 'None')}\")\n",
    "print(f\"\\nFound {len(structure_data.get('elements', []))} element types:\")\n",
    "\n",
    "for element in structure_data.get('elements', []):\n",
    "    print(f\"\\n**{element['element_type']}** (Level {element['level']})\")\n",
    "    print(f\"  Recognition: {element['recognition_pattern']}\")\n",
    "    print(f\"  Examples:\")\n",
    "    for example in element.get('examples', []):\n",
    "        print(f\"    - {example}\")\n",
    "\n",
    "# Store for next step\n",
    "document_structure = structure_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Header Extraction Per Page\n",
    "Use Gemini-2.0-flash-exp to extract actual headers from each page using the structure understanding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract headers from each page using Gemini-2.0-flash-exp and the structure understanding.\n",
    "\"\"\"\n",
    "\n",
    "### SCHEMA ###\n",
    "class PageHeader(BaseModel):\n",
    "    text: str = Field(..., description=\"The header text exactly as it appears\")\n",
    "    element_type: str = Field(..., description=\"Type of element based on structure analysis\")\n",
    "    level: int = Field(..., description=\"Hierarchical level (1=highest, 6=lowest)\")\n",
    "    confidence: str = Field(..., description=\"Confidence level: high, medium, low\")\n",
    "\n",
    "class PageHeaders(BaseModel):\n",
    "    page_number: int = Field(..., description=\"Page number (0-indexed)\")\n",
    "    headers: List[PageHeader] = Field(..., description=\"All headers found on this page\")\n",
    "\n",
    "### HELPER FUNCTIONS ###\n",
    "def create_extraction_prompt(structure_data: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Create a detailed prompt for header extraction based on structure analysis.\n",
    "    \n",
    "    Args:\n",
    "        structure_data: The document structure analysis results\n",
    "        \n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "    prompt = \"\"\"Based on the document structure analysis, extract all headers from this page.\n",
    "\n",
    "DOCUMENT STRUCTURE REFERENCE:\n",
    "\"\"\"\n",
    "    \n",
    "    for element in structure_data.get('elements', []):\n",
    "        prompt += f\"\\n**{element['element_type']}** (Level {element['level']})\"\n",
    "        prompt += f\"\\n  Recognition: {element['recognition_pattern']}\"\n",
    "        prompt += f\"\\n  Examples: {', '.join(element.get('examples', [])[:2])}\"\n",
    "        prompt += \"\\n\"\n",
    "    \n",
    "    prompt += \"\"\"\n",
    "TASK:\n",
    "1. Identify ALL headers on this page that match the structure patterns above\n",
    "2. For each header, determine its exact text, element type, and level\n",
    "3. Return results as structured JSON\n",
    "\n",
    "Be thorough - don't miss any headers, even if they're small or seem minor.\n",
    "\n",
    "Return EXACTLY this JSON structure (no extra text, no markdown formatting, just JSON):\n",
    "\n",
    "{\n",
    "  \"headers\": [\n",
    "    {\n",
    "      \"text\": \"exact header text as it appears\",\n",
    "      \"element_type\": \"main_title\",\n",
    "      \"level\": 1\n",
    "    },\n",
    "    {\n",
    "      \"text\": \"another header text\",\n",
    "      \"element_type\": \"section_header\",\n",
    "      \"level\": 2\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def extract_headers_from_page(page_image: str, page_num: int, extraction_prompt: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract headers from a single page using Gemini-2.0-flash-exp.\n",
    "    \n",
    "    Args:\n",
    "        page_image: Base64-encoded page image\n",
    "        page_num: Page number (0-indexed)\n",
    "        extraction_prompt: Prompt for header extraction\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with page headers\n",
    "    \"\"\"\n",
    "    # Create messages for single page\n",
    "    messages = create_vision_messages(\n",
    "        images=[page_image],\n",
    "        system_prompt=\"You are an expert at extracting headers from research paper pages. Use the provided structure analysis to identify all headers accurately.\",\n",
    "        user_prompt=extraction_prompt\n",
    "    )\n",
    "    \n",
    "    # Call Gemini-2.0-flash-exp\n",
    "    response = client.chat.completions.create(\n",
    "        model=EXTRACTION_MODEL,\n",
    "        messages=messages,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        max_tokens=1500,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    # Parse response with error handling\n",
    "    content = response.choices[0].message.content\n",
    "    \n",
    "    try:\n",
    "        page_data = json.loads(content)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"    JSON error on page {page_num + 1}: {e.msg}\")\n",
    "        \n",
    "        # Try to extract JSON from response\n",
    "        import re\n",
    "        json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "        if json_match:\n",
    "            try:\n",
    "                page_data = json.loads(json_match.group(0))\n",
    "                print(f\"    Successfully parsed extracted JSON for page {page_num + 1}\")\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"    Creating fallback empty result for page {page_num + 1}\")\n",
    "                page_data = {\"headers\": []}\n",
    "        else:\n",
    "            print(f\"    No JSON found, creating empty result for page {page_num + 1}\")\n",
    "            page_data = {\"headers\": []}\n",
    "    \n",
    "    # Validate and fix headers structure\n",
    "    if \"headers\" not in page_data:\n",
    "        print(f\"    Missing 'headers' field on page {page_num + 1}, creating empty list\")\n",
    "        page_data[\"headers\"] = []\n",
    "    \n",
    "    # Validate each header has required fields\n",
    "    valid_headers = []\n",
    "    for i, header in enumerate(page_data.get(\"headers\", [])):\n",
    "        if not isinstance(header, dict):\n",
    "            print(f\"    Header {i+1} on page {page_num + 1} is not a dict, skipping\")\n",
    "            continue\n",
    "            \n",
    "        # Check required fields\n",
    "        required_fields = [\"text\", \"element_type\", \"level\"]\n",
    "        missing_fields = [field for field in required_fields if field not in header or header[field] is None]\n",
    "        \n",
    "        if missing_fields:\n",
    "            print(f\"    Header {i+1} on page {page_num + 1} missing fields {missing_fields}, skipping\")\n",
    "            continue\n",
    "            \n",
    "        # Validate level is an integer\n",
    "        if not isinstance(header[\"level\"], int):\n",
    "            print(f\"    Header {i+1} on page {page_num + 1} has invalid level '{header['level']}', skipping\")\n",
    "            continue\n",
    "            \n",
    "        valid_headers.append(header)\n",
    "    \n",
    "    page_data[\"headers\"] = valid_headers\n",
    "    \n",
    "    # Ensure page number is set\n",
    "    page_data['page_number'] = page_num\n",
    "    \n",
    "    return page_data\n",
    "\n",
    "### MAIN PROCESSING ###\n",
    "extraction_prompt = create_extraction_prompt(document_structure)\n",
    "all_page_headers = []\n",
    "\n",
    "total_pages = len(all_page_images)\n",
    "print(f\"Extracting headers from {total_pages} pages using Gemini-2.0-flash-exp...\")\n",
    "\n",
    "# Process each page\n",
    "for page_num, page_image in enumerate(all_page_images):\n",
    "    print(f\"Processing page {page_num + 1}/{total_pages}...\")\n",
    "    \n",
    "    try:\n",
    "        page_headers = extract_headers_from_page(page_image, page_num, extraction_prompt)\n",
    "        all_page_headers.append(page_headers)\n",
    "        \n",
    "        headers_count = len(page_headers.get('headers', []))\n",
    "        print(f\"  Found {headers_count} headers on page {page_num + 1}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing page {page_num + 1}: {e}\")\n",
    "        # Add empty result to maintain page indexing\n",
    "        all_page_headers.append({\n",
    "            'page_number': page_num,\n",
    "            'headers': [],\n",
    "            'error': str(e)\n",
    "        })\n",
    "    \n",
    "    # Progress update every 5 pages\n",
    "    if (page_num + 1) % 5 == 0:\n",
    "        total_headers = sum(len(p.get('headers', [])) for p in all_page_headers)\n",
    "        print(f\"  Progress: {page_num + 1}/{total_pages} pages, {total_headers} headers total\")\n",
    "\n",
    "print(f\"\\nCompleted header extraction from all {total_pages} pages\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Results Display\n",
    "Print all extracted headers organized by page and hierarchy level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Display all extracted headers in a readable format.\n",
    "\"\"\"\n",
    "\n",
    "### RESULTS DISPLAY ###\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"                    DOCUMENT HEADER EXTRACTION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Summary statistics\n",
    "total_headers = 0\n",
    "headers_by_level = {}\n",
    "pages_with_headers = 0\n",
    "\n",
    "for page_data in all_page_headers:\n",
    "    headers = page_data.get('headers', [])\n",
    "    if headers:\n",
    "        pages_with_headers += 1\n",
    "    \n",
    "    for header in headers:\n",
    "        total_headers += 1\n",
    "        level = header.get('level', 0)\n",
    "        headers_by_level[level] = headers_by_level.get(level, 0) + 1\n",
    "\n",
    "print(f\"\\nSUMMARY:\")\n",
    "print(f\"  Total pages: {len(all_page_headers)}\")\n",
    "print(f\"  Pages with headers: {pages_with_headers}\")\n",
    "print(f\"  Total headers: {total_headers}\")\n",
    "print(f\"  Headers by level: {dict(sorted(headers_by_level.items()))}\")\n",
    "\n",
    "print(f\"\\nDETAILED RESULTS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Display headers for each page\n",
    "for page_data in all_page_headers:\n",
    "    page_num = page_data.get('page_number', 0)\n",
    "    headers = page_data.get('headers', [])\n",
    "    error = page_data.get('error')\n",
    "    \n",
    "    if error:\n",
    "        print(f\"\\nPage {page_num + 1}: ERROR - {error}\")\n",
    "        continue\n",
    "    \n",
    "    if not headers:\n",
    "        print(f\"\\nPage {page_num + 1}: No headers found\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nPage {page_num + 1}: {len(headers)} headers\")\n",
    "    \n",
    "    # Sort headers by level for better readability\n",
    "    sorted_headers = sorted(headers, key=lambda h: h.get('level', 99))\n",
    "    \n",
    "    for header in sorted_headers:\n",
    "        level = header.get('level', 0)\n",
    "        element_type = header.get('element_type', 'unknown')\n",
    "        text = header.get('text', '').strip()\n",
    "        \n",
    "        # Create indentation based on level\n",
    "        indent = \"  \" + \"  \" * (level - 1) if level > 0 else \"  \"\n",
    "        level_marker = f\"H{level}\" if level > 0 else \"??\"\n",
    "        \n",
    "        print(f\"{indent}[{level_marker}] {text}\")\n",
    "        print(f\"{indent}     Type: {element_type}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"                           EXTRACTION COMPLETE\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
