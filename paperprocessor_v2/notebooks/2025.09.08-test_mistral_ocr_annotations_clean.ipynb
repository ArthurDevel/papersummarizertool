{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral OCR: Clean Annotations + Headings Comparison\n",
    "\n",
    "This notebook runs a clean, readable pipeline:\n",
    "- Basic OCR to get per-page markdown and image bboxes (with crops)\n",
    "- Annotations to extract language, title, authors, chapter titles, URLs\n",
    "- Headings extracted two ways for comparison:\n",
    "  - From page markdown (ATX and setext syntax)\n",
    "  - From an OCR-inferred outline (model infers heading levels and pages)\n",
    "\n",
    "References: [Basic OCR](https://docs.mistral.ai/capabilities/document_ai/basic_ocr/), [Annotations](https://docs.mistral.ai/capabilities/document_ai/annotations/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Setup\n",
    "Configures the client, paths, and constants used across the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setup the Mistral client and define basic constants.\n",
    "- Keep code simple and explicit; raise if required variables are missing.\n",
    "\"\"\"\n",
    "\n",
    "# ### CONSTANTS ###\n",
    "from pathlib import Path\n",
    "NOTEBOOK_NAME: str = \"2025.09.08-test_mistral_ocr_annotations_clean\"\n",
    "PDF_PATH: Path = Path(\"/Users/Focus/Downloads/2212.14024v2.pdf\")\n",
    "MODEL: str = \"mistral-ocr-latest\"\n",
    "PAGES: list[int] = list(range(8))  # Document annotation supports up to 8 pages\n",
    "\n",
    "# ### DEPENDENCIES ###\n",
    "import os\n",
    "import base64\n",
    "from dotenv import load_dotenv\n",
    "from mistralai import Mistral\n",
    "\n",
    "# ### SETUP CLIENT ###\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"MISTRAL_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"MISTRAL_API_KEY is not set in environment.\")\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "if not PDF_PATH.exists():\n",
    "    raise FileNotFoundError(f\"PDF not found: {PDF_PATH}\")\n",
    "\n",
    "# Build a data: URL for the PDF (simple and explicit)\n",
    "with open(PDF_PATH, \"rb\") as f:\n",
    "    _pdf_bytes = f.read()\n",
    "DOCUMENT_SPEC = {\n",
    "    \"type\": \"document_url\",\n",
    "    \"document_url\": \"data:application/pdf;base64,\" + base64.b64encode(_pdf_bytes).decode(\"utf-8\"),\n",
    "}\n",
    "\n",
    "print(\"Ready. Model:\", MODEL)\n",
    "print(\"PDF:\", PDF_PATH)\n",
    "print(\"Pages (doc annotation scope):\", PAGES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Basic OCR (markdown + image crops)\n",
    "Runs Basic OCR to get per-page markdown and image bboxes. Also prints page count and shows cropped images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run Basic OCR and display results:\n",
    "- Print per-page markdown\n",
    "- Print bbox coordinates\n",
    "- Display each cropped bbox image\n",
    "- Print number of pages detected\n",
    "\"\"\"\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image as PILImage\n",
    "import io\n",
    "\n",
    "ocr = client.ocr.process(\n",
    "    model=MODEL,\n",
    "    document=DOCUMENT_SPEC,\n",
    "    include_image_base64=True,\n",
    ")\n",
    "\n",
    "print(\"Pages (len):\", len(ocr.pages))\n",
    "for page in ocr.pages:\n",
    "    dims = getattr(page, \"dimensions\", None)\n",
    "    print(f\"\\n## Page {page.index} | dims: {getattr(dims,'width',None)}x{getattr(dims,'height',None)} dpi={getattr(dims,'dpi',None)}\")\n",
    "    print(page.markdown)\n",
    "\n",
    "    images = getattr(page, \"images\", []) or []\n",
    "    if not images:\n",
    "        print(\"(no image bboxes)\")\n",
    "    for i, img in enumerate(images, start=1):\n",
    "        tlx = img.top_left_x\n",
    "        tly = img.top_left_y\n",
    "        brx = img.bottom_right_x\n",
    "        bry = img.bottom_right_y\n",
    "        w = brx - tlx\n",
    "        h = bry - tly\n",
    "        print(f\"- Image {i}: id={getattr(img,'id',None)} bbox=({tlx},{tly})â†’({brx},{bry}) size=({w}x{h})\")\n",
    "\n",
    "        data_str = img.image_base64\n",
    "        if not data_str:\n",
    "            continue\n",
    "        if data_str.startswith(\"data:\"):\n",
    "            _, b64_data = data_str.split(\",\", 1)\n",
    "        else:\n",
    "            b64_data = data_str\n",
    "        image_bytes = base64.b64decode(b64_data)\n",
    "        pil_img = PILImage.open(io.BytesIO(image_bytes))\n",
    "        display(pil_img)\n",
    "\n",
    "ocr_pages = ocr.pages  # used later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Annotations (language, title, authors, chapters, URLs)\n",
    "Requests document-level annotations and prints them in a readable format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract document-level fields via Document Annotation with batching.\n",
    "Processes the document in 8-page chunks (API limit) and combines results.\n",
    "- Schema: language, title, authors, chapter_titles, urls\n",
    "\"\"\"\n",
    "\n",
    "### IMPORTS ###\n",
    "from pydantic import BaseModel, Field\n",
    "from mistralai.extra import response_format_from_pydantic_model\n",
    "import json as _json\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "### SCHEMA ###\n",
    "class DocumentAnnotation(BaseModel):\n",
    "    language: str = Field(..., description=\"Language of the document\")\n",
    "    title: str | None = Field(None, description=\"Document title if present\")\n",
    "    authors: list[str] = Field(..., description=\"Author names\")\n",
    "    chapter_titles: list[str] = Field(..., description=\"Chapter titles in order\")\n",
    "    urls: list[str] = Field(..., description=\"URLs referenced in the document\")\n",
    "\n",
    "### HELPER FUNCTIONS ###\n",
    "def create_page_batches(total_pages: int, batch_size: int = 8) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Split pages into batches of specified size without overlap.\n",
    "    \n",
    "    Args:\n",
    "        total_pages: Total number of pages in document\n",
    "        batch_size: Maximum pages per batch (API limit is 8)\n",
    "        \n",
    "    Returns:\n",
    "        List of page number lists, e.g. [[0,1,2,3,4,5,6,7], [8,9,10,11,12,13,14]]\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    for start_page in range(0, total_pages, batch_size):\n",
    "        end_page = min(start_page + batch_size, total_pages)\n",
    "        batch_pages = list(range(start_page, end_page))\n",
    "        batches.append(batch_pages)\n",
    "    return batches\n",
    "\n",
    "def process_annotation_batch(client, model: str, document_spec: Dict, pages: List[int], response_format) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a single batch of pages for document annotation.\n",
    "    \n",
    "    Args:\n",
    "        client: Mistral client\n",
    "        model: Model name\n",
    "        document_spec: Document specification\n",
    "        pages: List of page numbers to process\n",
    "        response_format: Pydantic response format\n",
    "        \n",
    "    Returns:\n",
    "        Parsed annotation data as dictionary\n",
    "    \"\"\"\n",
    "    ann = client.ocr.process(\n",
    "        model=model,\n",
    "        document=document_spec,\n",
    "        pages=pages,\n",
    "        document_annotation_format=response_format,\n",
    "        include_image_base64=False,\n",
    "    )\n",
    "\n",
    "    raw = ann.document_annotation\n",
    "    if isinstance(raw, str):\n",
    "        return _json.loads(raw)\n",
    "    elif hasattr(raw, \"model_dump\"):\n",
    "        return raw.model_dump()\n",
    "    elif isinstance(raw, dict):\n",
    "        return raw\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported document_annotation type: {type(raw)}\")\n",
    "\n",
    "def combine_annotations(batch_results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Combine annotation results from multiple batches.\n",
    "    \n",
    "    Args:\n",
    "        batch_results: List of annotation dictionaries from each batch\n",
    "        \n",
    "    Returns:\n",
    "        Combined annotation dictionary\n",
    "    \"\"\"\n",
    "    if not batch_results:\n",
    "        raise ValueError(\"No batch results to combine\")\n",
    "    \n",
    "    # Step 1: Take language and title from first batch (should be consistent)\n",
    "    first_batch = batch_results[0]\n",
    "    combined = {\n",
    "        \"language\": first_batch.get(\"language\"),\n",
    "        \"title\": first_batch.get(\"title\"),\n",
    "    }\n",
    "    \n",
    "    # Step 2: Combine authors from all batches (remove duplicates, preserve order)\n",
    "    all_authors = []\n",
    "    seen_authors = set()\n",
    "    for batch in batch_results:\n",
    "        for author in batch.get(\"authors\", []):\n",
    "            if author.lower() not in seen_authors:\n",
    "                all_authors.append(author)\n",
    "                seen_authors.add(author.lower())\n",
    "    combined[\"authors\"] = all_authors\n",
    "    \n",
    "    # Step 3: Combine chapter titles in order across all batches\n",
    "    all_chapter_titles = []\n",
    "    for batch in batch_results:\n",
    "        all_chapter_titles.extend(batch.get(\"chapter_titles\", []))\n",
    "    combined[\"chapter_titles\"] = all_chapter_titles\n",
    "    \n",
    "    # Step 4: Combine URLs from all batches (remove duplicates)\n",
    "    all_urls = []\n",
    "    seen_urls = set()\n",
    "    for batch in batch_results:\n",
    "        for url in batch.get(\"urls\", []):\n",
    "            if url not in seen_urls:\n",
    "                all_urls.append(url)\n",
    "                seen_urls.add(url)\n",
    "    combined[\"urls\"] = all_urls\n",
    "    \n",
    "    return combined\n",
    "\n",
    "### MAIN PROCESSING ###\n",
    "doc_rf = response_format_from_pydantic_model(DocumentAnnotation)\n",
    "\n",
    "# Step 1: Create page batches based on total pages from OCR\n",
    "total_pages = len(ocr_pages)\n",
    "page_batches = create_page_batches(total_pages, batch_size=8)\n",
    "print(f\"Processing {total_pages} pages in {len(page_batches)} batches:\")\n",
    "for i, batch in enumerate(page_batches):\n",
    "    print(f\"  Batch {i+1}: pages {batch[0]}-{batch[-1]} ({len(batch)} pages)\")\n",
    "\n",
    "# Step 2: Process each batch\n",
    "batch_results = []\n",
    "for i, batch_pages in enumerate(page_batches):\n",
    "    print(f\"Processing batch {i+1}/{len(page_batches)}...\")\n",
    "    batch_result = process_annotation_batch(\n",
    "        client=client,\n",
    "        model=MODEL,\n",
    "        document_spec=DOCUMENT_SPEC,\n",
    "        pages=batch_pages,\n",
    "        response_format=doc_rf\n",
    "    )\n",
    "    batch_results.append(batch_result)\n",
    "\n",
    "# Step 3: Combine results from all batches\n",
    "parsed = combine_annotations(batch_results)\n",
    "\n",
    "# Step 4: Display final results\n",
    "print(\"\\n=== COMBINED DOCUMENT ANNOTATIONS ===\")\n",
    "print(\"language:\", parsed.get(\"language\"))\n",
    "print(\"title:\", parsed.get(\"title\"))\n",
    "print(\"authors:\")\n",
    "for a in parsed.get(\"authors\", []):\n",
    "    print(\" -\", a)\n",
    "print(\"chapter_titles:\")\n",
    "for t in parsed.get(\"chapter_titles\", []):\n",
    "    print(\" -\", t)\n",
    "print(\"urls:\")\n",
    "for u in parsed.get(\"urls\", []):\n",
    "    print(\" -\", u)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Headings from Markdown (by page)\n",
    "Extracts headings from each pageâ€™s markdown using ATX and setext rules, and records page index + line number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parse headings from per-page markdown.\n",
    "- ATX: lines starting with 1..6 '#' characters\n",
    "- Setext: lines followed by '===' or '---' underlines\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "markdown_headings: List[Dict[str, Any]] = []\n",
    "for page in ocr_pages:\n",
    "    page_index = page.index\n",
    "    lines = (page.markdown or \"\").splitlines()\n",
    "\n",
    "    # ATX headers\n",
    "    for i, line in enumerate(lines, start=1):\n",
    "        m = re.match(r\"^(#{1,6})\\s+(.*)$\", line)\n",
    "        if m:\n",
    "            level = len(m.group(1))\n",
    "            text = m.group(2).strip()\n",
    "            markdown_headings.append({\n",
    "                \"source\": \"markdown\",\n",
    "                \"page_index\": page_index,\n",
    "                \"line\": i,\n",
    "                \"level\": level,\n",
    "                \"text\": text,\n",
    "            })\n",
    "\n",
    "    # Setext headers\n",
    "    for i in range(2, len(lines) + 1):\n",
    "        underline = lines[i - 1].strip()\n",
    "        if re.match(r\"^={3,}$\", underline):\n",
    "            markdown_headings.append({\n",
    "                \"source\": \"markdown\",\n",
    "                \"page_index\": page_index,\n",
    "                \"line\": i - 1,\n",
    "                \"level\": 1,\n",
    "                \"text\": lines[i - 2].strip(),\n",
    "            })\n",
    "        elif re.match(r\"^-{3,}$\", underline):\n",
    "            markdown_headings.append({\n",
    "                \"source\": \"markdown\",\n",
    "                \"page_index\": page_index,\n",
    "                \"line\": i - 1,\n",
    "                \"level\": 2,\n",
    "                \"text\": lines[i - 2].strip(),\n",
    "            })\n",
    "\n",
    "print(f\"Found {len(markdown_headings)} markdown headings\")\n",
    "for h in markdown_headings:\n",
    "    print(f\"[page {h['page_index']} line {h['line']}] h{h['level']}: {h['text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Headings from OCR-Inferred Outline\n",
    "Requests a structured outline as an annotation, where the model infers heading levels and page indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Request a structured outline as a document annotation with batching.\n",
    "Processes the document in 8-page chunks (API limit) and combines outlines.\n",
    "- The model infers heading level (1..6) only. Page indices are not requested.\n",
    "\"\"\"\n",
    "\n",
    "### IMPORTS ###\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Any\n",
    "from mistralai.extra import response_format_from_pydantic_model\n",
    "import json as _json\n",
    "\n",
    "### SCHEMA ###\n",
    "class OutlineItem(BaseModel):\n",
    "    title: str = Field(..., description=\"Heading text\")\n",
    "    level: int = Field(..., description=\"Heading level 1..6\")\n",
    "\n",
    "class DocumentOutline(BaseModel):\n",
    "    outline: List[OutlineItem] = Field(..., description=\"Document outline\")\n",
    "\n",
    "### HELPER FUNCTIONS ###\n",
    "def process_outline_batch(client, model: str, document_spec: Dict, pages: List[int], response_format) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process a single batch of pages for outline extraction.\n",
    "    \n",
    "    Args:\n",
    "        client: Mistral client\n",
    "        model: Model name\n",
    "        document_spec: Document specification\n",
    "        pages: List of page numbers to process\n",
    "        response_format: Pydantic response format\n",
    "        \n",
    "    Returns:\n",
    "        List of outline items from this batch\n",
    "    \"\"\"\n",
    "    outline_resp = client.ocr.process(\n",
    "        model=model,\n",
    "        document=document_spec,\n",
    "        pages=pages,\n",
    "        document_annotation_format=response_format,\n",
    "        include_image_base64=False,\n",
    "    )\n",
    "\n",
    "    raw_outline = outline_resp.document_annotation\n",
    "    if isinstance(raw_outline, str):\n",
    "        outline_parsed = _json.loads(raw_outline)\n",
    "    elif hasattr(raw_outline, \"model_dump\"):\n",
    "        outline_parsed = raw_outline.model_dump()\n",
    "    elif isinstance(raw_outline, dict):\n",
    "        outline_parsed = raw_outline\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported outline type: {type(raw_outline)}\")\n",
    "    \n",
    "    return outline_parsed.get(\"outline\", [])\n",
    "\n",
    "def combine_outlines(batch_outline_results: List[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Combine outline results from multiple batches.\n",
    "    \n",
    "    Args:\n",
    "        batch_outline_results: List of outline item lists from each batch\n",
    "        \n",
    "    Returns:\n",
    "        Combined list of outline items in order\n",
    "    \"\"\"\n",
    "    if not batch_outline_results:\n",
    "        raise ValueError(\"No batch outline results to combine\")\n",
    "    \n",
    "    # Step 1: Combine all outline items in order\n",
    "    combined_outline = []\n",
    "    for batch_outline in batch_outline_results:\n",
    "        combined_outline.extend(batch_outline)\n",
    "    \n",
    "    return combined_outline\n",
    "\n",
    "### MAIN PROCESSING ###\n",
    "outline_rf = response_format_from_pydantic_model(DocumentOutline)\n",
    "\n",
    "# Step 1: Reuse page batches from document annotation processing\n",
    "# (page_batches was already created in cell 6)\n",
    "print(f\"Processing outline for {len(page_batches)} batches:\")\n",
    "\n",
    "# Step 2: Process each batch for outline\n",
    "batch_outline_results = []\n",
    "for i, batch_pages in enumerate(page_batches):\n",
    "    print(f\"Processing outline batch {i+1}/{len(page_batches)}...\")\n",
    "    batch_outline = process_outline_batch(\n",
    "        client=client,\n",
    "        model=MODEL,\n",
    "        document_spec=DOCUMENT_SPEC,\n",
    "        pages=batch_pages,\n",
    "        response_format=outline_rf\n",
    "    )\n",
    "    batch_outline_results.append(batch_outline)\n",
    "    print(f\"  Found {len(batch_outline)} headings in batch {i+1}\")\n",
    "\n",
    "# Step 3: Combine outline results from all batches\n",
    "ocr_outline = combine_outlines(batch_outline_results)\n",
    "\n",
    "# Step 4: Display final results\n",
    "print(f\"\\n=== COMBINED OCR OUTLINE ===\")\n",
    "print(f\"Found {len(ocr_outline)} outline headings total\")\n",
    "for item in ocr_outline:\n",
    "    print(f\"h{item.get('level')}: {item.get('title')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Compare Headings (Markdown vs OCR-Inferred)\n",
    "Shows both heading lists side-by-side (printed), so you can visually compare consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Print both headings lists for a quick manual comparison.\n",
    "- Markdown headings include page and line\n",
    "- OCR outline includes only level and title (no page)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nMarkdown-derived headings:\")\n",
    "for h in markdown_headings:\n",
    "    print(f\"[page {h['page_index']} line {h['line']}] h{h['level']}: {h['text']}\")\n",
    "\n",
    "print(\"\\nOCR-inferred outline headings:\")\n",
    "for item in ocr_outline:\n",
    "    print(f\"h{item.get('level')}: {item.get('title')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Align OCR Outline to Markdown Lines (RapidFuzz)\n",
    "Matches each OCR-inferred outline heading to the closest markdown heading on the same page to assign an exact markdown line number. This preserves the modelâ€™s semantic levels while grounding to precise locations.\n",
    "\n",
    "Note: Requires `rapidfuzz`. If not installed, install it in your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Align OCR outline items to markdown headings using RapidFuzz.\n",
    "Simplified:\n",
    "- Ignore any page indices from the outline (not trustworthy).\n",
    "- Match each OCR outline title against ALL markdown headings in the document.\n",
    "- Assign page and line purely from the best markdown match.\n",
    "Saves `headers_index_normalized.json` with aligned results.\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "from rapidfuzz import fuzz\n",
    "import json as _json\n",
    "from pathlib import Path as _Path\n",
    "\n",
    "def normalize_heading_text(text: str) -> str:\n",
    "    \"\"\"Lowercase and collapse whitespace for robust matching.\"\"\"\n",
    "    return \" \".join((text or \"\").lower().split())\n",
    "\n",
    "# Build a flat list of markdown headings across the whole document\n",
    "all_md: List[Dict[str, Any]] = list(markdown_headings)\n",
    "\n",
    "aligned: List[Dict[str, Any]] = []\n",
    "THRESHOLD: int = 85\n",
    "\n",
    "for item in ocr_outline:\n",
    "    title = str(item.get(\"title\") or \"\")\n",
    "    level = int(item.get(\"level\"))\n",
    "\n",
    "    if not all_md:\n",
    "        aligned.append({\n",
    "            \"page_index\": None,\n",
    "            \"ocr_title\": title,\n",
    "            \"ocr_level\": level,\n",
    "            \"markdown_line\": None,\n",
    "            \"markdown_title\": None,\n",
    "            \"score\": None,\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    query = normalize_heading_text(title)\n",
    "\n",
    "    best_score = -1\n",
    "    best_idx = None\n",
    "    for idx, md_h in enumerate(all_md):\n",
    "        score = fuzz.token_set_ratio(query, normalize_heading_text(md_h[\"text\"]))\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_idx = idx\n",
    "\n",
    "    if best_idx is not None and best_score >= THRESHOLD:\n",
    "        md_h = all_md[best_idx]\n",
    "        aligned.append({\n",
    "            \"page_index\": md_h[\"page_index\"],\n",
    "            \"ocr_title\": title,\n",
    "            \"ocr_level\": level,\n",
    "            \"markdown_line\": md_h[\"line\"],\n",
    "            \"markdown_title\": md_h[\"text\"],\n",
    "            \"score\": int(best_score),\n",
    "        })\n",
    "    else:\n",
    "        aligned.append({\n",
    "            \"page_index\": None,\n",
    "            \"ocr_title\": title,\n",
    "            \"ocr_level\": level,\n",
    "            \"markdown_line\": None,\n",
    "            \"markdown_title\": None,\n",
    "            \"score\": int(best_score) if best_score >= 0 else None,\n",
    "        })\n",
    "\n",
    "print(f\"Aligned {len(aligned)} outline items\")\n",
    "for a in aligned:\n",
    "    print(f\"[page {a['page_index']}] h{a['ocr_level']} â†’ line {a['markdown_line']} (score={a['score']}): {a['ocr_title']}\")\n",
    "\n",
    "# Save outputs with filename prefix\n",
    "out_dir = _Path.cwd() / \"outputs\" \n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "out_path = out_dir / f\"{NOTEBOOK_NAME}_headers_index_normalized.json\"\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    _json.dump(aligned, f, ensure_ascii=False, indent=2)\n",
    "print(\"Saved:\", out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
