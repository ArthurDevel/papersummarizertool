# Database-Only JSON Storage Plan

## Problem
Papers processed by Airflow on Railway are not appearing in the dashboard because Airflow and the main app cannot share file system volumes on Railway. Currently, processed paper JSON is stored in `data/paperjsons/{uuid}.json` files.

## Solution
Store the complete processed paper JSON directly in the `papers` database table, eliminating the need for shared file system access.

---

## Implementation Steps

### 1. Database Migration ✅ COMPLETED

**File:** `migrations/versions/20251006_000022_add_processed_content_to_papers.py`

Add new column to `papers` table:
- Column name: `processed_content`
- Type: `LONGTEXT` (MySQL) - supports up to 4GB
- Nullable: `True` (for backward compatibility)
- Purpose: Store complete paper JSON as string

```python
def upgrade():
    op.add_column('papers', sa.Column('processed_content', sa.Text().with_variant(LONGTEXT, 'mysql'), nullable=True))

def downgrade():
    op.drop_column('papers', 'processed_content')
```

---

### 2. Update Database Model ✅ COMPLETED

**File:** `papers/db/models.py`

Add the new column to `PaperRecord`:

```python
from sqlalchemy.dialects.mysql import LONGTEXT

class PaperRecord(Base):
    # ... existing fields ...
    processed_content = Column(Text().with_variant(LONGTEXT, 'mysql'), nullable=True)
```

---

### 3. Update `save_paper()` Function ✅ COMPLETED

**File:** `papers/client.py`

Modify the `save_paper()` function to:
1. Generate the JSON dictionary (keep existing logic)
2. Store JSON as string in `processed_content` column
3. Remove file system write operations

**Changes:**
- Remove: File writing to `get_processed_result_path()`
- Add: Store `json.dumps(result_dict)` to `paper_data['processed_content']`

```python
def save_paper(db: Session, processed_content: ProcessedDocument) -> Paper:
    # ... existing conversion logic to result_dict ...
    
    # Step 3: Store JSON in database (not as file)
    json_string = _json.dumps(result_dict, ensure_ascii=False)
    
    # Step 4: Create or update paper record in database
    paper_data = {
        'paper_uuid': paper_uuid,
        'arxiv_id': arxiv_id,
        'title': processed_content.title,
        'authors': processed_content.authors,
        'status': 'completed',
        'num_pages': len(processed_content.pages),
        'total_cost': result_dict["total_cost"],
        'avg_cost_per_page': result_dict["avg_cost_per_page"],
        'thumbnail_data_url': result_dict["thumbnail_data_url"],
        'processed_content': json_string,  # NEW
        'finished_at': datetime.utcnow(),
    }
    
    if is_update:
        record = update_paper_record(db, paper_uuid, paper_data)
    else:
        record = create_paper_record(db, paper_data)
    
    return Paper.model_validate(record)
```

---

### 4. Update `get_paper()` Function ✅ COMPLETED

**File:** `papers/client.py`

Modify to read from database column:

```python
def get_paper(db: Session, paper_uuid: str) -> Paper:
    """
    Get complete paper by UUID.
    Loads both database metadata and full processing results from database.
    
    Returns:
        Paper: Complete paper with metadata and content (pages, sections)
        
    Raises:
        FileNotFoundError: If paper with UUID not found or no processed content
    """
    # Step 1: Get database record
    record = get_paper_record(db, paper_uuid)
    
    # Step 2: Check for processed content in database
    if not record.processed_content:
        raise FileNotFoundError(f"No processed content found for paper {paper_uuid}")
    
    # Step 3: Parse JSON from database
    try:
        result_dict = _json.loads(record.processed_content)
    except Exception as e:
        raise RuntimeError(f"Failed to parse processed content for paper {paper_uuid}: {e}")
    
    # Step 4: Convert legacy JSON format to our DTOs (keep existing logic)
    # ... existing conversion code ...
```

---

### 5. Update Frontend API Endpoint ✅ COMPLETED

**File:** `frontend/app/layouttests/data/route.ts`

Modify to fetch from backend API instead of file system:

```typescript
export async function GET(request: Request) {
  try {
    const url = new URL(request.url);
    const requestedFile = url.searchParams.get('file');

    if (!requestedFile) {
      // Return list of available papers from API
      const response = await fetch(`${process.env.NEXT_PUBLIC_API_URL}/admin/papers?status=completed`);
      const papers = await response.json();
      const files = papers.map((p: any) => `${p.paper_uuid}.json`);
      return NextResponse.json({ files }, { status: 200 });
    }

    // Extract UUID from filename
    const uuid = requestedFile.replace('.json', '');
    
    // Fetch paper from API
    const response = await fetch(`${process.env.NEXT_PUBLIC_API_URL}/papers/${uuid}`);
    if (!response.ok) {
      throw new Error(`Paper not found: ${response.status}`);
    }
    
    const data = await response.json();
    return NextResponse.json(data, { status: 200 });
  } catch (error) {
    const message = error instanceof Error ? error.message : 'Unknown error';
    return NextResponse.json({ error: `Failed to fetch paper: ${message}` }, { status: 500 });
  }
}
```

---

### 6. Add Backend API Endpoint for Paper Retrieval ✅ COMPLETED

**File:** `api/endpoints/paper_processing_endpoints.py`

Add endpoint to retrieve complete paper JSON:

```python
@router.get("/papers/{paper_uuid}")
def get_paper_json(paper_uuid: UUID, db: Session = Depends(get_session)):
    """
    Get complete paper JSON including all processed content.
    """
    try:
        paper = papers_client.get_paper(db, str(paper_uuid))
        # Convert Paper DTO to legacy JSON format for frontend
        # (implementation matches what get_paper() returns)
        return paper.to_dict()  # or manually construct dict
    except FileNotFoundError:
        raise HTTPException(status_code=404, detail="Paper not found")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

---

### 7. Data Migration Script ✅ COMPLETED

**Files:** 
- `migrations/scripts/migrate_json_files_to_db.py` - Migration script
- `migrations/scripts/README.md` - Documentation for scripts directory
- `entrypoint.sh` - Updated to run all scripts automatically

Create script to migrate existing JSON files into database:

```python
"""
Migrate existing paper JSON files from filesystem to database.

Usage: python -m migrations.scripts.migrate_json_files_to_db
"""

import os
import json
from sqlalchemy.orm import Session
from shared.db import SessionLocal
from papers.db.models import PaperRecord
from papers.client import get_processed_result_path

def migrate_json_files_to_database():
    """
    Read all JSON files from data/paperjsons/ and store in database.
    """
    db: Session = SessionLocal()
    
    try:
        # Get all papers from database
        papers = db.query(PaperRecord).all()
        
        migrated_count = 0
        skipped_count = 0
        error_count = 0
        
        for paper in papers:
            # Skip if already has processed_content
            if paper.processed_content:
                print(f"Skipping {paper.paper_uuid} - already has content in DB")
                skipped_count += 1
                continue
            
            # Try to load JSON file
            json_path = get_processed_result_path(paper.paper_uuid)
            
            if not os.path.exists(json_path):
                print(f"Warning: No JSON file found for {paper.paper_uuid}")
                error_count += 1
                continue
            
            try:
                with open(json_path, 'r', encoding='utf-8') as f:
                    json_content = f.read()
                
                # Validate JSON
                json.loads(json_content)
                
                # Store in database
                paper.processed_content = json_content
                db.add(paper)
                
                print(f"Migrated {paper.paper_uuid}")
                migrated_count += 1
                
            except Exception as e:
                print(f"Error migrating {paper.paper_uuid}: {e}")
                error_count += 1
        
        # Commit all changes
        db.commit()
        
        print(f"\nMigration complete:")
        print(f"  Migrated: {migrated_count}")
        print(f"  Skipped: {skipped_count}")
        print(f"  Errors: {error_count}")
        
    finally:
        db.close()

if __name__ == "__main__":
    migrate_json_files_to_database()
```

---

### 8. Optimize Database Queries ✅ COMPLETED

Updated all queries to defer loading the heavy `processed_content` column by default.

**Changes:**
- Added `defer(PaperRecord.processed_content)` to all queries that don't need the full content
- Explicitly load content with `load_content=True` or `undefer()` only where needed:
  - `get_paper()` - loads full paper with content
  - `get_paper_json()` API endpoint - returns full JSON
  - Migration script - needs to check and write content

**Files updated:**
- `papers/db/client.py` - Added `load_content` parameter to `get_paper_record()`
- `papers/client.py` - Loads content explicitly in `get_paper()`
- `api/endpoints/paper_processing_endpoints.py` - Defers content in metadata queries
- `api/endpoints/search.py` - Defers content in search queries
- `api/endpoints/admin.py` - Defers content in admin queries
- `users/client.py` - Defers content in user list queries
- `migrations/scripts/20251006_migrate_json_files_to_db.py` - Explicitly loads content

**Performance impact:**
- List queries (admin dashboard, search) no longer load multi-MB JSON blobs
- Metadata-only operations are much faster
- Only 2 endpoints load the full content when needed

---

## Deployment Steps

### Step 1: Deploy to Server
1. Push code to repository
2. Server pulls latest code
3. Entrypoint automatically runs:
   - Database schema migration: `alembic upgrade head` (adds `processed_content` column)
   - All data migration scripts in `migrations/scripts/` (migrates existing JSON files)
4. Application starts with database-only storage

### Step 2: Verify Migration (First Deploy)
1. Check that existing papers have `processed_content` populated in database
2. Verify dashboard shows existing papers
3. Process a new paper and verify it's stored in database

### Step 3: Cleanup (Second Deploy)
1. On next server restart, data migration script automatically:
   - Detects JSON files already have content in database
   - Deletes the local JSON files
   - Exits cleanly

### Step 4: Subsequent Deploys
1. Data migration script exits early (no files to migrate)
2. No overhead on startup

---

## Testing Checklist

- [ ] Migration creates `processed_content` column successfully
- [ ] Data migration script transfers existing JSON files to database
- [ ] New papers are saved to database (not files)
- [ ] Dashboard shows migrated papers correctly
- [ ] Dashboard shows newly processed papers
- [ ] Paper viewing page works for both old and new papers
- [ ] Search functionality works correctly
- [ ] Performance is acceptable (JSON retrieval from DB)

---

## Rollback Plan

If issues arise:

1. **Immediate:** Keep JSON files during transition period
2. **Fallback:** Update `get_paper()` to check file system if DB column is NULL
3. **Full Rollback:** Revert code changes and continue using file system

---

## Benefits

✅ Works with Railway's no-shared-volume architecture  
✅ Cleaner architecture - single source of truth  
✅ Easier backups (database only)  
✅ No file sync issues  
✅ Atomic transactions (JSON and metadata together)  

## Potential Concerns

⚠️ **Database size:** Large JSONs increase database size (mitigated by LONGTEXT compression)  
⚠️ **Query performance:** Loading large text fields (mitigated by selective column loading)  
⚠️ **Migration time:** Existing papers need migration (one-time operation)

## Performance Optimization (Future)

If database size becomes an issue:
- Add compression for `processed_content` (gzip before storing)
- Use lazy loading (don't load processed_content unless needed)
- Consider separate `paper_content` table if needed
