
Goal: implement vector search for the following cases:
1) the user has a search query as input, we process this search query (see below) and return the most similar papers
2) the user lands on a paper. Using the vector of this paper and possible filters (see below), we return the most similar papers


## User Search

STEP 1: The user has a search box in which they type a query (eg. "I am looking for recent papers related to optimizing vector search for RAG")

STEP 2: The query gets processed, these things happen in parallel:
a) an LLM selects the appropriate categories for this query, using shared/arxiv/internals/arxiv_categories.csv. These are returned as a list
b) an LLM selects the appropriate date range for this query. In this instance, the user asks for recent papers, so we set it to "from 1 year ago to now"
c) an LLM rewrites the user query 

important: step a and b only happens if it is a fully new search query. We thus need a way to detect if this search query is new. Probably it is best that this is a variable in the frontend, which is passed along. Reason: if we have already set categories and date ranges but just want to adapt the query, we don't want it to change every time again. Also, it is possible that the user does not want a date filter or category filter. 

STEP 3: With these filters and the new query, we do a vector search. You can connect with our Qdrant vector db using the .env variables QDRANT_HOST, QDRANT_PORT, QDRANT_API_KEY (these are already set). To do this vector search, we use voyage api, model voyage-3.5. Tge VOYAGE_API_KEY is already set in .env, we still need to make shared/voyageai/client.py.
docs: https://docs.voyageai.com/reference/embeddings-api

STEP 4: After this, we use a reranker (rerank-2.5-lite) also from voyage. This rerank needs to be added too to shared/voyageai/client.py
docs: https://docs.voyageai.com/reference/reranker-api 

STEP 5: Finally, the results are returned


## Paper Search

STEP 1: We pass the Arxiv paper id to the backend

STEP 2: I think the backend can use this id to find the vector in the vector database? 

STEP 3: Now with this vector, we do a vector search and return the results

## Technical
- all prompts are stored in shared/arxiv/prompts
- we are using openrouter for the llm calls (shared/openrouter/client.py) with model 'openai/gpt-oss-120b'


## Implementation

STEP 1: Add missing config keys
- Edit `shared/config.py` to include new settings and defaults:
```python
class Settings(BaseSettings):
    # ...existing...
    VOYAGE_API_KEY: str
    QDRANT_HOST: str
    QDRANT_PORT: int = 6333
    QDRANT_API_KEY: str
```
- Add to your `.env` (local and server):
```env
VOYAGE_API_KEY=sk_...
QDRANT_HOST=localhost   # use "qdrant" in docker-compose network
QDRANT_PORT=6333
QDRANT_API_KEY=your_qdrant_api_key_or_blank_if_none
```
- Verify the app boots and logs include the new envs (no crash on Settings load).

STEP 2: Create VoyageAI client and models
- Files: `shared/voyageai/models.py`, `shared/voyageai/client.py`.
- `models.py` (pydantic):
  - `EmbeddingResult`: fields `model: str`, `vectors: list[list[float]]`.
  - `RerankItem`: `index: int`, `score: float`.
  - `RerankResult`: `model: str`, `items: list[RerankItem]`.
- `client.py`:
  - Read `VOYAGE_API_KEY` from settings; set base URL; reuse one async HTTP client.
  - `async def embed_texts(texts: list[str], model: str = "voyage-3.5", input_type: str = "document") -> EmbeddingResult`.
  - `async def embed_query(text: str, model: str = "voyage-3.5") -> list[float]` (wrapper around `embed_texts`).
  - `async def rerank(query: str, documents: list[str], top_k: int = 20, model: str = "rerank-2.5-lite") -> RerankResult`.
  - Handle batching, timeouts, and HTTP errors clearly (raise with context).
  - Note: Look up embedding dimension for `voyage-3.5` in docs; do not hardcode without checking.

STEP 3: Create Qdrant client and models
- Files: `shared/qdrant/models.py`, `shared/qdrant/client.py`.
- `models.py` (pydantic payload for each vector):
  - `PaperVectorPayload`: `paper_uuid: str`, `arxiv_id: str`, `slug: str | None`, `title: str | None`, `authors: str | None`, `categories: list[str] | None`, `published_at: str | None`, `updated_at: str | None`.
- `client.py`:
  - Initialize a singleton qdrant client using `QDRANT_HOST`, `QDRANT_PORT`, `QDRANT_API_KEY`.
  - `ensure_collection(name: str = "papers", vector_size: int, distance: str = "Cosine")` – create if missing (set proper size once you confirm Voyage dim).
  - `upsert_paper_vector(paper_uuid: str, vector: list[float], payload: PaperVectorPayload)`.
  - `search_by_vector(query_vector: list[float], limit: int = 50, categories: list[str] | None = None, date_from: str | None = None, date_to: str | None = None)` – build filter by `categories` terms and ISO dates against `published_at`/`updated_at`.
  - Optional: `get_vector_by_id(paper_uuid: str)` if needed for paper-similar.

STEP 4: Create search endpoints and types
- Files: `api/types/search.py`, `api/endpoints/search.py`.
- `api/types/search.py` (pydantic):
  - `SearchQueryRequest`: `query: str`, `is_new: bool`, `selected_categories: list[str] | None`, `date_from: str | None`, `date_to: str | None`, `limit: int | None`.
  - `SearchItem`: `paper_uuid: str`, `slug: str | None`, `title: str | None`, `authors: str | None`, `thumbnail_data_url: str | None`, `score: float | None`.
  - `SearchQueryResponse`: `items: list[SearchItem]`, `applied_categories: list[str] | None`, `applied_date_from: str | None`, `applied_date_to: str | None`, `rewritten_query: str | None`.
  - `SimilarPapersResponse`: `items: list[SearchItem]`.
- `api/endpoints/search.py` (FastAPI router):
  - `POST /search/query`:
    1) If `is_new` is true, call OpenRouter LLM to suggest categories/date range and rewrite the query using `shared/arxiv/internals/arxiv_categories.csv` (load file into prompt).
    2) Embed the (rewritten or original) query via Voyage.
    3) Qdrant search with filters.
    4) Rerank top-N candidates using Voyage reranker with `query` vs candidate texts (e.g., `title + "\n\n" + abstract if available`).
    5) Map to `SearchItem` (augment slug via DB lookup and thumb via minimal scan if needed).
  - `GET /search/paper/{paper_uuid}/similar` with `limit`, `categories`, `date_from`, `date_to`:
    1) Retrieve the paper’s stored vector from Qdrant (or by keeping local copy).
    2) Qdrant search (exclude the same `paper_uuid`).
    3) Rerank using the paper’s `title+abstract` as query string.
    4) Return `SimilarPapersResponse`.
  - Register router in `api/main.py` with `app.include_router(search.router, tags=["search"])`.

STEP 5: Implement the processing pipeline
- When a paper becomes available (e.g., in `import_paper_json` after DB write):
  - Fetch arXiv metadata if needed (`title`, `abstract`, `categories`, `published/updated` dates) using `shared/arxiv/client.py`.
  - Build an embedding document: `title + "\n\n" + abstract` (truncate if very long).
  - Generate embedding via Voyage and call `ensure_collection` once at startup with the confirmed vector size.
  - Upsert to Qdrant with payload: `paper_uuid`, `arxiv_id`, `slug` (if present), `title`, `authors`, `categories`, `published_at`, `updated_at`.
- Add an admin endpoint (optional) to reindex all papers into Qdrant.
- Log and continue on non-fatal embedding/Qdrant errors so the main import succeeds.

STEP 6: Implement the frontend
- Create a new page `frontend/app/search/page.tsx`:
  - Components: search input, `isNewQuery` toggle, category multiselect (from CSV), date range pickers, submit button.
  - Call `POST /api/search/query` via new functions in `frontend/services/api.ts` and render results list (title, authors, thumb, link to `/paper/[slug]`).
- Update `frontend/services/api.ts` with:
  - `searchPapers(req: SearchQueryRequest): Promise<SearchQueryResponse>`.
  - `getSimilarPapers(paperUuid: string, params): Promise<SimilarPapersResponse>`.
- Replace the “Similar papers” sidebar in `frontend/app/paper/[slug]/page.tsx` to call the new similar endpoint instead of listing all other files.
- Preserve existing minimal list behavior as a fallback if search is unavailable.