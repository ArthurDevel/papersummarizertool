{
  "paper_id": "temp_id",
  "title": "INTRODUCTION",
  "sections": [
    {
      "level": 1,
      "section_title": "INTRODUCTION",
      "start_page": 1,
      "end_page": 1,
      "rewritten_content": "## 1 INTRODUCTION\n\nThe Transformer is a popular and important type of deep learning model used in many different areas of artificial intelligence, including:\n*   **Natural Language Processing (NLP):** Understanding and working with human language.\n*   **Computer Vision (CV):** Interpreting and processing images.\n*   **Speech Processing:** Analyzing and understanding spoken language.\n\nOriginally, the Transformer was created to handle tasks that convert one sequence to another, such as machine translation. Since then, it has become the standard choice for **Pre-trained Models (PTMs)**, which are models trained on a large amount of data that can then be fine-tuned for specific tasks. This has led to top-tier performance, especially in language-related tasks. The Transformer's use has also expanded to other fields like chemistry and life sciences.\n\nBecause of its success, many variations of the Transformer have been created. These are often called **X-formers**. They aim to improve on the original (\"vanilla\") Transformer in several ways. We can group these improvements into three main categories:\n\n1.  **Model Efficiency:** A major challenge with the original Transformer is that it becomes very slow and uses a lot of memory when dealing with long inputs (like long documents). Improvements focus on making the core \"self-attention\" mechanism more lightweight or using \"divide-and-conquer\" methods to break down large problems into smaller, more manageable ones.\n\n2.  **Model Generalization:** The Transformer is very flexible and doesn't make many assumptions about the data it's given. This means it needs a huge amount of data to learn effectively and can struggle with smaller datasets. Improvements here involve guiding the model's learning by adding specific rules or constraints, or by pre-training it on massive amounts of general data before fine-tuning it on a specific task.\n\n3.  **Model Adaptation:** This category includes efforts to modify the Transformer to make it work better for specific, specialized tasks and applications.\n\nIn this survey, we provide a detailed review of the Transformer and its many variants (X-formers). While we organize them by the categories above, it's important to note that many X-formers address more than one of these issues at the same time. For example, some new attention methods not only reduce computing requirements but also help the model learn better by adding structural information.",
      "summary": "The Transformer is a highly successful deep learning model, originally developed for machine translation but now widely used in natural language processing, computer vision, audio processing, and other scientific fields. Its success has led to the creation of numerous variants, often called X-formers.\n\nThis paper surveys these variants, categorizing them based on the improvements they make to the original Transformer. The main areas of improvement are:\n1.  **Model Efficiency:** Addressing the high computational and memory costs of the self-attention mechanism, especially for long sequences, using techniques like lightweight attention and divide-and-conquer methods.\n2.  **Model Generalization:** Improving performance on smaller datasets by introducing structural biases, regularization, or leveraging large-scale pre-training.\n3.  **Model Adaptation:** Modifying the architecture for specific downstream tasks and applications.",
      "subsections": []
    },
    {
      "level": 1,
      "section_title": "BACKGROUND",
      "start_page": 2,
      "end_page": 5,
      "rewritten_content": "## 2. BACKGROUND\n\nThis section provides an overview of the fundamental concepts behind the Transformer architecture.\n\n### 2.1 Vanilla Transformer\n\nThe \"vanilla\" or original Transformer is a model designed to transform an input sequence into an output sequence, making it suitable for tasks like machine translation. It consists of two main parts: an **encoder** and a **decoder**.\n\n*   **Encoder:** This part processes the input sequence (e.g., a sentence in English). It is made up of a stack of identical layers.\n*   **Decoder:** This part generates the output sequence (e.g., the translated sentence in French). It is also made up of a stack of identical layers.\n\nEach layer in both the encoder and decoder contains two key sub-components: a **multi-head self-attention** module and a **position-wise feed-forward network (FFN)**. To help the model train more effectively, especially in deep networks, a \"residual connection\" and \"layer normalization\" are applied around each of these two sub-components.\n\nThe decoder has one additional component: a **cross-attention module**. This module allows the decoder to look at the output of the encoder, which is crucial for tasks like translation where the output needs to be based on the input.\n\nThe overall architecture is shown in the diagram below (Fig. 1).\n\n![Overview of vanilla Transformer architecture](/-/media/api/project/664369a4746d2a0013b5e1b2/24f7e2713f0c2a00138dcc44?w=529&h=496&version=0)\n*Fig. 1. An overview of the original Transformer architecture.*\n\n#### 2.1.1 Attention Modules\n\nThe core idea of the Transformer is the **attention mechanism**. It allows the model to weigh the importance of different words in the input sequence when processing a specific word. This is based on a Query-Key-Value (QKV) model.\n\n*   **Query (Q):** Represents the current word or position being processed.\n*   **Key (K):** Represents all the words in the sequence that the current word can pay attention to.\n*   **Value (V):** Also represents all the words in the sequence. These are the values that get combined to form the output.\n\nThe model calculates attention by:\n1.  Measuring the similarity between the **Query** and every **Key** using a dot product.\n2.  Scaling the result by the square root of the key's dimension (`d_k`) to stabilize training.\n3.  Applying a `softmax` function to turn these scores into positive weights that sum to 1.\n4.  Multiplying these weights by the **Values** to create a weighted sum. This result becomes the output for the current position.\n\nThe formula for this \"scaled dot-product attention\" is:\n`Attention(Q, K, V) = softmax( (Q * K^T) / sqrt(d_k) ) * V`\n\nInstead of performing attention just once, Transformers use **multi-head attention**. The model splits the queries, keys, and values into multiple smaller pieces called \"heads.\" It then performs the attention calculation for each head in parallel. Finally, the outputs from all heads are combined to produce the final result. This allows the model to focus on different aspects of the input sequence simultaneously.\n\nThere are three main types of attention used in a Transformer, depending on where the Queries, Keys, and Values come from:\n\n*   **Self-attention:** Used in the encoder. The Q, K, and V all come from the output of the previous layer in the encoder. This allows each word in the input sequence to look at all other words in the same sequence.\n*   **Masked Self-attention:** Used in the decoder. It is similar to self-attention, but with a \"mask\" applied. This mask prevents a position from attending to subsequent (future) positions. This is essential for generation tasks, ensuring the model only uses previous words to predict the next word. This is also called \"causal\" or \"autoregressive\" attention.\n*   **Cross-attention:** Used in the decoder. The Queries come from the decoder's previous layer, while the Keys and Values come from the output of the encoder. This is the step where the decoder pays attention to the input sequence to generate an appropriate output.\n\n#### 2.1.2 Position-wise FFN\n\nThe **Position-wise Feed-Forward Network (FFN)** is the other main component in each Transformer block. It is a simple neural network that is applied independently to each position (e.g., each word) in the sequence. It consists of two linear transformations with a ReLU activation function in between. This helps the model process the information gathered by the attention mechanism.\n\n#### 2.1.3 Residual Connection and Normalization\n\nTo build a deep and stable model, each sub-layer (the attention module and the FFN) is wrapped with two additional operations:\n\n1.  **Residual Connection:** The input to the sub-layer is added to the output of the sub-layer. This helps prevent the \"vanishing gradient\" problem during training, allowing information to flow more easily through the network.\n2.  **Layer Normalization:** This operation normalizes the output of the sub-layer, which stabilizes the training process and often speeds it up.\n\nSo, the output of each sub-layer is calculated as: `LayerNorm(X + Sublayer(X))`, where `X` is the input and `Sublayer` is either the attention or the FFN module.\n\n#### 2.1.4 Position Encodings\n\nUnlike recurrent neural networks (RNNs), Transformers do not process data in a sequential order. They look at the entire sequence at once. Because of this, the model has no inherent sense of word order. To fix this, we add \"positional encodings\" to the input embeddings. These are vectors that provide information about the position of each word in the sequence, allowing the model to understand word order.\n\n### 2.2 Model Usage\n\nThe Transformer architecture is flexible and can be used in several ways, depending on the task:\n\n*   **Encoder-Decoder:** The full, original architecture is used for sequence-to-sequence tasks like machine translation or text summarization.\n*   **Encoder-only:** Only the encoder part is used. The output is a rich representation of the input sequence. This is useful for classification tasks (e.g., sentiment analysis) or question answering. Models like BERT are encoder-only.\n*   **Decoder-only:** Only the decoder part is used, without the cross-attention module. This is ideal for generating sequences, such as in language modeling, where the goal is to predict the next word in a sentence. Models like GPT are decoder-only.\n\n### 2.3 Model Analysis\n\nTo understand the model's performance, we can analyze the computational requirements of its two main components: self-attention and the position-wise FFN.\n\n| Module | Complexity (Operations) | # Parameters |\n| :--- | :--- | :--- |\n| self-attention | O(T² ⋅ D) | 8D² |\n| position-wise FFN | O(T ⋅ D²) | 8D² |\n\n*In this table, `T` is the length of the sequence and `D` is the dimension of the model's hidden layers.*\n\n*   For **short sequences**, the FFN (with complexity proportional to `D²`) is often the computational bottleneck.\n*   For **long sequences**, the self-attention mechanism (with complexity proportional to `T²`) becomes the main bottleneck. The memory required to store the attention scores grows quadratically with the sequence length, making it challenging to use Transformers for very long sequences like high-resolution images or long documents.\n\n### 2.4 Comparing Transformer to Other Network Types\n\n#### 2.4.1 Analysis of Self-Attention\n\nSelf-attention is a key innovation of the Transformer. Here is how it compares to other common layer types used in deep learning:\n\n| Layer Type | Complexity per Layer | Sequential Operations | Maximum Path Length |\n| :--- | :--- | :--- | :--- |\n| **Self-Attention** | O(T² ⋅ D) | O(1) | O(1) |\n| **Fully Connected** | O(T² ⋅ D²) | O(1) | O(1) |\n| **Convolutional** | O(k ⋅ T ⋅ D²) | O(1) | O(log<sub>k</sub>(T)) |\n| **Recurrent** | O(T ⋅ D²) | O(T) | O(T) |\n\nKey advantages of self-attention are:\n1.  **Modeling Long-Range Dependencies:** The \"maximum path length\" between any two positions in the sequence is `O(1)`. This means the model can directly relate words that are very far apart, which is difficult for Recurrent and Convolutional layers where the path length grows with the sequence length.\n2.  **Parallelization:** The computations within a self-attention layer are highly parallelizable (the number of sequential operations is constant, `O(1)`), unlike Recurrent layers, which must process sequences step-by-step.\n\n#### 2.4.2 In Terms of Inductive Bias\n\n**Inductive bias** refers to the assumptions a model makes about the data to generalize from training examples to new ones.\n\n*   **Convolutional Neural Networks (CNNs)** have a strong inductive bias. They assume *locality* (nearby pixels are related) and *translation invariance* (a pattern is the same no matter where it appears).\n*   **Recurrent Neural Networks (RNNs)** also have a strong inductive bias, assuming that data has a *temporal* or sequential structure.\n*   **Transformers** have a very weak inductive bias. They don't make strong assumptions about how input data is structured. This makes them extremely flexible and powerful but can also lead to overfitting on small datasets. Transformers learn relationships based solely on the data itself through the self-attention mechanism.\n\nCompared to **Graph Neural Networks (GNNs)**, which operate on explicitly defined graph structures, a Transformer has no prior knowledge of the input's structure. It infers all relationships dynamically based on similarity measures between input elements.",
      "summary": "The vanilla Transformer is an encoder-decoder model where each component is a stack of identical blocks. Each block contains a multi-head self-attention module and a position-wise feed-forward network (FFN), with residual connections and layer normalization. The core of the model is scaled dot-product attention, which operates on Query, Key, and Value (QKV) representations. Three types of attention are used: self-attention in the encoder, masked self-attention in the decoder to maintain causality, and cross-attention to link the decoder to the encoder's output.\n\nSince Transformers lack inherent recurrence, positional encodings are added to the input to model sequence order. The architecture can be used in full encoder-decoder, encoder-only, or decoder-only configurations. A key limitation is the self-attention mechanism's computational complexity, which scales quadratically with sequence length (O(T²·D)), making it inefficient for long sequences. Unlike CNNs or RNNs, the Transformer has a weak inductive bias, making it highly flexible but prone to overfitting on small datasets. Its main advantages are a constant path length for modeling long-range dependencies and high parallelizability.",
      "subsections": [
        {
          "level": 2,
          "section_title": "Vanilla Transformer",
          "start_page": 2,
          "end_page": 4,
          "rewritten_content": null,
          "summary": null,
          "subsections": [
            {
              "level": 3,
              "section_title": "Attention Modules",
              "start_page": 2,
              "end_page": 2,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            },
            {
              "level": 3,
              "section_title": "Position-wise FFN",
              "start_page": 3,
              "end_page": 3,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            },
            {
              "level": 3,
              "section_title": "Residual Connection and Normalization",
              "start_page": 4,
              "end_page": 4,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            },
            {
              "level": 3,
              "section_title": "Position Encodings",
              "start_page": 4,
              "end_page": 4,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            }
          ]
        },
        {
          "level": 2,
          "section_title": "Model Usage",
          "start_page": 4,
          "end_page": 4,
          "rewritten_content": null,
          "summary": null,
          "subsections": []
        },
        {
          "level": 2,
          "section_title": "Model Analysis",
          "start_page": 4,
          "end_page": 4,
          "rewritten_content": null,
          "summary": null,
          "subsections": []
        },
        {
          "level": 2,
          "section_title": "Comparing Transformer to Other Network Types",
          "start_page": 5,
          "end_page": 5,
          "rewritten_content": null,
          "summary": null,
          "subsections": [
            {
              "level": 3,
              "section_title": "Analysis of Self-Attention",
              "start_page": 5,
              "end_page": 5,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            },
            {
              "level": 3,
              "section_title": "In terms of Inductive Bias",
              "start_page": 5,
              "end_page": 5,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            }
          ]
        }
      ]
    },
    {
      "level": 1,
      "section_title": "TAXONOMY OF TRANSFORMERS",
      "start_page": 6,
      "end_page": 6,
      "rewritten_content": "## 3. Taxonomy of Transformers\n\nOver time, many different versions of the original Transformer model have been created. We can group these \"Transformer variants\" based on three main aspects:\n1.  **Architecture Modifications:** Changes to the model's internal structure.\n2.  **Pre-training Methods:** How the model is initially trained.\n3.  **Applications:** The specific tasks the model is used for.\n\nThe diagram in the original document (Fig. 2) visually organizes these categories. Architecture modifications, which are the main focus of this paper, can be divided into two levels:\n\n*   **Module-level:** Changes to the core building blocks of the Transformer. This includes modifications to:\n    *   The **Attention** mechanism.\n    *   The **Position-wise Feed-Forward Network (FFN)**.\n    *   The **Normalization** layers.\n    *   The **Positional Encodings** that help the model understand sequence order.\n*   **Architecture-level:** Changes to the overall model structure, like how the blocks are connected or organized.\n\n### How This Paper Is Organized\n\nThis paper focuses specifically on the **architecture modifications**. We will explore these changes in the following sections:\n*   **Section 4:** Describes variants related to the attention mechanism.\n*   **Section 5:** Introduces other changes at the module level.\n*   **Section 6:** Details changes at the higher architecture level.\n*   **Section 7 & 8:** Briefly cover pre-training methods and applications.\n\nFor a deeper dive into Pre-trained Models (PTMs) and Visual Transformers, other comprehensive survey papers are available.",
      "summary": "Transformer model variants are categorized based on three perspectives: architecture modifications, pre-training methods, and applications.\n\nArchitecture modifications are the primary focus and are divided into two types:\n1.  **Module-level:** Changes to internal components of the Transformer block, such as the self-attention mechanism, position-wise feed-forward network (FFN), layer normalization, and positional encodings. A key goal is to overcome the quadratic complexity of self-attention using techniques like sparse or linearized attention.\n2.  **Architecture-level:** Modifications to the overall model structure, including lightweight variants, cross-block connectivity, recurrence, and hierarchy.\n\nThe other categories include pre-training methods (Encoder, Decoder, Encoder-Decoder) and applications across various domains like Text, Vision, Audio, and Multi-modal data.",
      "subsections": []
    },
    {
      "level": 1,
      "section_title": "ATTENTION",
      "start_page": 6,
      "end_page": 20,
      "rewritten_content": "## 4 ATTENTION\n\nThe \"self-attention\" mechanism is a core part of what makes Transformers so powerful. It allows the model to weigh the importance of different words in an input sequence when processing a specific word. However, the standard self-attention mechanism faces two main challenges in practice:\n\n1.  **Complexity**: The computational cost grows very rapidly as the length of the input text increases. Specifically, it has a complexity of `O(T²)`, where `T` is the sequence length. This makes it slow and memory-intensive for long documents or high-resolution images.\n2.  **Lack of Structural Prior**: The mechanism starts with no built-in assumptions about the structure of the data. For example, it doesn't automatically know that words closer to each other are often more related. It has to learn all these relationships from scratch, which can be difficult, especially with limited training data.\n\nTo address these issues, researchers have developed several improvements. These improvements generally fall into several categories:\n\n*   **(1) Sparse Attention**: Makes the attention mechanism more efficient by having each word only focus on a smaller, selected subset of other words, instead of all of them.\n*   **(2) Linearized Attention**: Changes the mathematical calculation of attention to achieve linear complexity (`O(T)`) instead of quadratic (`O(T²)`).\n*   **(3) Query Prototyping and Memory Compression**: Reduces computation by using representative \"prototype\" queries or by compressing the information that attention looks at.\n*   **(4) Low-rank Self-Attention**: Simplifies the attention calculation based on the observation that the full attention matrix often has a simpler, underlying structure.\n*   **(5) Attention with Prior**: Gives the model helpful hints or \"priors\" about where it should focus its attention.\n*   **(6) Improved Multi-Head Mechanism**: Enhances how the different parallel attention mechanisms (\"heads\") work together.\n\nThis section will describe these different attention variants in more detail.\n\n### 4.1 Sparse Attention\n\nIn standard self-attention, every word (or token) in a sequence must attend to every other token. This creates a dense attention matrix where all possible pairs are scored. However, for many tasks, this is unnecessary, and the resulting attention matrix is often \"sparse\" (meaning most scores are near zero).\n\nSparse attention methods take advantage of this by pre-defining or learning a sparse pattern. This means each token only attends to a limited number of other tokens, which reduces computation and memory usage.\n\nWe can categorize these methods based on how the sparse connections are determined: either by the position of tokens or their content.\n\n#### 4.1.1 Position-based Sparse Attention\n\nIn this approach, the pattern of which tokens attend to which other tokens is fixed and based on their positions in the sequence. These pre-defined patterns can be broken down into fundamental \"atomic\" patterns, which are often combined into more complex \"compound\" patterns.\n\n##### 4.1.1.1 Atomic Sparse Attention\n\nThese are the basic building blocks for creating sparse attention patterns. The five main types are:\n\n*   **(1) Global Attention**: A few special tokens (called \"global nodes\" or \"hub nodes\") are allowed to attend to every other token in the sequence. This ensures information can travel across the entire sequence.\n*   **(2) Band Attention (or Sliding Window Attention)**: Each token only attends to its immediate neighbors within a fixed-size window. This is effective because, in many types of data (like text), nearby elements are most relevant to each other.\n*   **(3) Dilated Attention (or Strided Attention)**: Similar to band attention, but it attends to neighbors with gaps or \"dilations\". This allows the model to have a larger receptive field (see farther away) without increasing computational cost.\n*   **(4) Random Attention**: To increase the connections beyond a local window, each token also attends to a few randomly selected tokens from the sequence.\n*   **(5) Block Local Attention**: The sequence is divided into several non-overlapping blocks, and attention is only calculated within each block.\n\n##### 4.1.1.2 Compound Sparse Attention\n\nMore advanced sparse attention models often combine several atomic patterns. For example:\n\n*   **Star-Transformer**: Combines *band attention* with *global attention*.\n*   **Longformer**: Also combines *band attention* with *global attention*.\n*   **Extended Transformer Construction (ETC)**: Utilizes a combination of *band attention* and *global-local attention*.\n*   **BigBird**: Uses a combination of *random attention*, *window attention*, and *global attention* to create a robust sparse model.\n\n##### 4.1.1.3 Extended Sparse Attention\n\nSome sparse patterns are specifically designed for certain types of data:\n\n*   **For Text**: The **BP-Transformer** arranges tokens in a binary tree structure. Each token attends to its parent, siblings, and children in the tree, allowing for structured, long-range connections.\n*   **For Images**: Since images are 2D, attention patterns can be structured differently. The **Image Transformer** applies sparse patterns like *block local attention* in 2D. The **Axial Transformer** calculates attention along each axis (height, then width) of the image separately, which is much more efficient than attending to all pixels at once.\n\n#### 4.1.2 Content-based Sparse Attention\n\nInstead of using fixed positions, this approach decides which tokens should attend to each other based on their actual content or meaning. The goal is to connect tokens that are most similar, even if they are far apart.\n\n*   **Clustering**: The **Routing Transformer** groups similar queries and keys into clusters. Attention is then only calculated between queries and keys that belong to the same cluster.\n*   **Locality-Sensitive Hashing (LSH)**: The **Reformer** uses a technique called LSH to quickly find pairs of keys and queries that are likely to be similar, without having to compare all of them. Attention is only performed on these likely pairs.\n*   **Learned Connections**: **Sparse Adaptive Connection (SAC)** learns to form connections between tokens. **Sparse Sinkhorn Attention** learns a sparse connection pattern and combines it with local attention to model data more effectively.\n\n### 4.2 Linearized Attention\n\nThe main bottleneck in standard attention is the computation of the `(QKᵀ)V` matrix, which involves a large intermediate matrix (`QKᵀ`) that has a size of `T x T` (where `T` is the sequence length).\n\nLinearized attention cleverly reorders this calculation to `Q(KᵀV)`. By computing `KᵀV` first, it avoids creating the large `T x T` matrix. This is possible by using a *kernel feature map* (`φ`), which is a function that approximates the dot-product similarity. This change reduces the complexity from quadratic (`O(T²)`) to linear (`O(T)`), making it much faster for long sequences.\n\nThere are two key components to this approach: the choice of the feature map and the rule used for aggregation.\n\n#### 4.2.1 Feature Maps\n\nThe feature map `φ` is a function that transforms the queries and keys so that their dot product approximates the original attention score. Different models propose different functions:\n\n*   **Linear Transformer**: Uses a simple feature map, `φ(x) = elu(x)+1`.\n*   **Performer**: Uses more sophisticated random feature maps that can approximate the standard attention mechanism very well. It uses trigonometric functions to create an unbiased approximation.\n*   Other models have explored different feature maps for specific purposes or to improve stability.\n\n#### 4.2.2 Aggregation Rule\n\nAfter applying the feature maps, the results need to be aggregated (summed up). While a simple sum works, it can sometimes be a bottleneck for the model's performance.\n\n*   The **RFA** model introduces a *gating mechanism* to the aggregation step. This mechanism learns to control how new information is added to the memory, allowing the model to forget old, irrelevant context and focus on what's important.\n\n### 4.3 Query Prototyping and Memory Compression\n\nAnother way to reduce the complexity of attention is to reduce the number of items involved in the calculation. This can be done by either reducing the number of queries or by compressing the keys and values.\n\n#### 4.3.1 Attention with Prototype Queries\n\nInstead of every token generating a \"query\" to seek information, this method uses a smaller set of \"prototype\" queries that represent groups of original queries.\n\n*   **Clustered Attention**: Groups all queries into several clusters and computes a single, shared attention distribution for each cluster.\n*   **Informer**: Selects only the most important, \"dominant\" queries to participate in the attention calculation, filtering out the rest.\n\n#### 4.3.2 Attention with Compressed Key-Value Memory\n\nInstead of reducing the number of queries, this approach reduces the number of keys and values that the queries must look through. It creates a compressed summary of the key-value information.\n\n*   **Linformer**: Reduces the sequence length of the keys and values by projecting them into a smaller dimension.\n*   **Poolformer**: Replaces the attention mechanism with a much simpler \"pooling\" operation, similar to what is used in Convolutional Neural Networks (CNNs), to compress information from neighboring tokens.\n\n### 4.4 Low-rank Self-Attention\n\nThis approach is based on the observation that the full `T x T` attention matrix is often \"low-rank.\" This means that although the matrix is large, its information can be represented much more compactly. Imagine a large image that is just a simple gradient; you don't need to store every pixel, just the start color, end color, and direction. Low-rank methods exploit this idea.\n\n#### 4.4.1 Low-rank Parameterization\n\nThis method directly models the attention matrix with a low-rank structure, which uses fewer parameters and is faster to compute. It essentially builds a \"bottleneck\" into the attention calculation.\n\n#### 4.4.2 Low-rank Approximation\n\nThis method aims to find a good low-rank approximation of the full attention matrix. A popular technique is the **Nyström method**, which selects a smaller number of \"landmark\" tokens. The full attention matrix is then approximated based on the attention scores of just these landmark tokens.\n\n### 4.5 Attention with Prior\n\nInstead of letting the model learn all attention patterns from scratch, this approach gives it a \"prior\"—a pre-defined pattern or hint about where it should focus. This prior attention is then combined with the standard, generated attention. This helps guide the model, especially when training data is limited.\n\n#### 4.5.1 Prior that Models Locality\n\nThis type of prior gives a hint that nearby tokens are more important. A common method is to add a *Gaussian bias*, which gives a higher probability to tokens that are closer to the current token being processed.\n\n#### 4.5.2 Prior from Lower Modules\n\nIn a deep Transformer model with many layers, the attention patterns learned by the lower layers can serve as a useful prior for the higher layers. The final attention score in a layer can be a weighted sum of its own generated attention and the attention from the layer below it.\n\n#### 4.5.3 Prior as Multi-task Adapters\n\nThis approach uses small, separate \"adapter\" modules that are trained to generate attention priors for specific tasks. This allows the model to adapt to new tasks more efficiently.\n\n#### 4.5.4 Attention with Only Prior\n\nSome methods go a step further and completely replace the dynamically generated attention with a fixed, non-trainable prior. For example, some models use a simple \"average\" attention or a Gaussian-based distribution. These are often much simpler and faster, and can be surprisingly effective for some tasks.\n\n### 4.6 Improved Multi-Head Mechanism\n\nStandard Transformers use \"multi-head attention,\" where several attention mechanisms (or \"heads\") run in parallel, each learning different patterns. This section discusses ways to improve how these heads work and interact.\n\n#### 4.6.1 Head Behavior Modeling\n\nA basic goal of multi-head attention is for different heads to capture different features. Some methods explicitly encourage this diversity by adding a regularization term during training that pushes the attention patterns of different heads to be different from one another.\n\n#### 4.6.2 Multi-head with Restricted Spans\n\nIt's often beneficial for some heads to focus on local context while others focus on the global context. This method assigns different attention spans (window sizes) to different heads. For example, lower layers might have heads with smaller spans, while higher layers have heads with larger spans, creating a hierarchy of features.\n\n#### 4.6.3 Multi-head with Refined Aggregation\n\nAfter each head computes its output, the results are typically concatenated and projected. Some work has explored more sophisticated ways to combine these outputs, such as using \"capsule networks\" or \"iterative routing\" to better aggregate the information from the different heads.\n\n#### 4.6.4 Other Modifications\n\n*   **Multi-query Attention**: To reduce memory and increase speed, this method has all heads share the same set of key and value vectors, while each head still learns its own unique query vector. This is effective because the model still has the flexibility to \"ask\" different questions (queries) while looking at the same source of information (keys and values).",
      "summary": "To address the quadratic complexity and lack of structural prior in standard self-attention, this section reviews several categories of improvements.\n\n**Sparse Attention** reduces computational complexity from O(T^2) by having each token attend to only a limited subset of other tokens. This sparsity can be position-based, using pre-defined patterns like sliding windows (band), dilated steps, or global nodes, or it can be content-based, using methods like locality-sensitive hashing (Reformer) or clustering to group similar items.\n\n**Linearized Attention** achieves linear complexity by reformulating the attention calculation. It uses kernel feature maps to approximate the softmax function, reordering matrix multiplications to avoid explicitly computing the T×T attention matrix.\n\nOther key approaches include:\n*   **Query Prototyping and Memory Compression:** Reduces complexity by either grouping queries into prototypes or compressing the number of key-value pairs.\n*   **Low-Rank Self-Attention:** Exploits the observation that the attention matrix is often low-rank by using factorization or approximation methods like Nyström.\n*   **Attention with Prior:** Injects inductive bias by combining the dynamically computed attention scores with a pre-defined prior distribution, which can enforce locality or other structural assumptions.\n*   **Improved Multi-Head Mechanisms:** Enhances standard multi-head attention by encouraging diversity among heads, assigning different heads to focus on different attention spans, or using more sophisticated methods to aggregate head outputs.",
      "subsections": [
        {
          "level": 2,
          "section_title": "Sparse Attention",
          "start_page": 8,
          "end_page": 10,
          "rewritten_content": null,
          "summary": null,
          "subsections": [
            {
              "level": 3,
              "section_title": "Position-based Sparse Attention",
              "start_page": 8,
              "end_page": 10,
              "rewritten_content": null,
              "summary": null,
              "subsections": [
                {
                  "level": 4,
                  "section_title": "Atomic Sparse Attention",
                  "start_page": 8,
                  "end_page": 8,
                  "rewritten_content": null,
                  "summary": null,
                  "subsections": []
                },
                {
                  "level": 4,
                  "section_title": "Compound Sparse Attention",
                  "start_page": 9,
                  "end_page": 9,
                  "rewritten_content": null,
                  "summary": null,
                  "subsections": []
                },
                {
                  "level": 4,
                  "section_title": "Extended Sparse Attention",
                  "start_page": 10,
                  "end_page": 10,
                  "rewritten_content": null,
                  "summary": null,
                  "subsections": []
                }
              ]
            },
            {
              "level": 3,
              "section_title": "Content-based Sparse Attention",
              "start_page": 10,
              "end_page": 11,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            }
          ]
        },
        {
          "level": 2,
          "section_title": "Linearized Attention",
          "start_page": 11,
          "end_page": 13,
          "rewritten_content": null,
          "summary": null,
          "subsections": [
            {
              "level": 3,
              "section_title": "Feature Maps",
              "start_page": 12,
              "end_page": 12,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            },
            {
              "level": 3,
              "section_title": "Aggregation Rule",
              "start_page": 13,
              "end_page": 13,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            }
          ]
        },
        {
          "level": 2,
          "section_title": "Query Prototyping and Memory Compression",
          "start_page": 14,
          "end_page": 14,
          "rewritten_content": null,
          "summary": null,
          "subsections": [
            {
              "level": 3,
              "section_title": "Attention with Prototype Queries",
              "start_page": 14,
              "end_page": 14,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            },
            {
              "level": 3,
              "section_title": "Attention with Compressed Key-Value Memory",
              "start_page": 14,
              "end_page": 14,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            }
          ]
        },
        {
          "level": 2,
          "section_title": "Low-rank Self-Attention",
          "start_page": 15,
          "end_page": 15,
          "rewritten_content": null,
          "summary": null,
          "subsections": [
            {
              "level": 3,
              "section_title": "Low-rank Parameterization",
              "start_page": 15,
              "end_page": 15,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            },
            {
              "level": 3,
              "section_title": "Low-rank Approximation",
              "start_page": 15,
              "end_page": 15,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            }
          ]
        },
        {
          "level": 2,
          "section_title": "Attention with Prior",
          "start_page": 15,
          "end_page": 17,
          "rewritten_content": null,
          "summary": null,
          "subsections": [
            {
              "level": 3,
              "section_title": "Prior that Models Locality",
              "start_page": 16,
              "end_page": 16,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            },
            {
              "level": 3,
              "section_title": "Prior from Lower Modules",
              "start_page": 16,
              "end_page": 16,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            },
            {
              "level": 3,
              "section_title": "Prior as Multi-task Adapters",
              "start_page": 17,
              "end_page": 17,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            },
            {
              "level": 3,
              "section_title": "Attention with Only Prior",
              "start_page": 17,
              "end_page": 17,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            }
          ]
        },
        {
          "level": 2,
          "section_title": "Improved Multi-Head Mechanism",
          "start_page": 18,
          "end_page": 20,
          "rewritten_content": null,
          "summary": null,
          "subsections": [
            {
              "level": 3,
              "section_title": "Head Behavior Modeling",
              "start_page": 18,
              "end_page": 18,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            },
            {
              "level": 3,
              "section_title": "Multi-head with Restricted Spans",
              "start_page": 18,
              "end_page": 18,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            },
            {
              "level": 3,
              "section_title": "Multi-head with Refined Aggregation",
              "start_page": 19,
              "end_page": 19,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            },
            {
              "level": 3,
              "section_title": "Other Modifications",
              "start_page": 20,
              "end_page": 20,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            }
          ]
        }
      ]
    },
    {
      "level": 1,
      "section_title": "OTHER MODULE-LEVEL MODIFICATIONS",
      "start_page": 20,
      "end_page": 26,
      "rewritten_content": "## 5 OTHER MODULE-LEVEL MODIFICATIONS\n\nThis section covers various improvements made to the individual components (or \"modules\") of the Transformer architecture, separate from the attention mechanism itself.\n\n### 5.1 Position Representations\n\nStandard Transformer models don't naturally understand the order of words in a sentence. They treat input as a \"bag of words,\" where order doesn't matter. To fix this, we need to explicitly add information about each word's position. This process is called adding \"positional representations\" or \"positional encodings.\"\n\n#### 5.1.1 Absolute Position Representations\n\nThis method gives each position in the input a unique code or vector.\n\n*   **Sinusoidal Functions:** The original Transformer used a clever method with sine and cosine functions to create a unique positional code for each spot in a sequence. This method has a fixed formula.\n*   **Learned Embeddings:** An alternative approach is to let the model *learn* the best positional code for each position during training. This is more flexible and can adapt to the specific data. However, a major drawback is that the model can only handle sequences as long as the ones it saw during training.\n\nWhile adding these positional codes to the initial input is common, some research suggests it's more effective to add them at every layer of the Transformer.\n\n#### 5.1.2 Relative Position Representations\n\nInstead of focusing on a word's absolute position (e.g., \"5th word\"), this approach focuses on the distance or relationship between pairs of words (e.g., \"this word is 3 spots away from that one\"). This can be more effective because the relative arrangement of words is often more important than their exact location.\n\n*   **Shaw et al.** proposed learning a representation for the distance between words and incorporating it directly into the attention calculation.\n*   **Transformer-XL** uses a similar but simpler sinusoidal encoding to represent relative positions, which is then added to the attention scores.\n*   **DeBERTa** builds on this by adding a learned relative position vector directly to the content embeddings before the attention calculation, separating the representation into content-to-content and position-to-content information.\n\n#### 5.1.3 Other Representations\n\nSome research combines different positional strategies. For example, the **Transformer with Untied Position Encoding (TUPE)** uses a hybrid approach. It represents attention scores as a combination of:\n*   Content-to-content interaction\n*   Absolute position-to-position interaction\n*   A \"bias\" term representing relative positional relationships\n\n#### 5.1.4 Position Representations without Explicit Encoding\n\nThese methods build positional awareness directly into the model's mechanics, rather than adding separate position vectors.\n\n*   **Rotary Position Embedding (RoPE):** Used in models like the Reformer, RoPE rotates the word embeddings based on their absolute position. When the model calculates attention between two words, their final attention score naturally depends only on their relative positions.\n*   **R-Transformer:** This model generalizes word embeddings by combining them with embeddings from a simpler, sequential model (a local RNN). This provides positional context without needing a separate positional encoding.\n*   **Conditional Position Encoding (CPE):** This approach, often used in Vision Transformers (ViT), learns to generate positional information using a 2D convolution. This is more dynamic than fixed encodings.\n\n#### 5.1.5 Position Representation on Transformer Decoders\n\nInterestingly, some studies have found that for the decoder part of a Transformer, which generates output, removing positional encodings can sometimes improve performance on certain language modeling tasks.\n\n### 5.2 Layer Normalization\n\nLayer Normalization (LN) is a technique used in Transformers to stabilize the training process and help the model learn more effectively. This section discusses where to place LN and what alternatives exist.\n\n#### 5.2.1 Placement of Layer Normalization\n\nThere are two primary ways to place the LN layer within a Transformer block, as shown in the diagram below.\n\n\n*(a) post-LN: Normalization is applied *after* the attention and feed-forward operations. (b) pre-LN: Normalization is applied *before* the operations.*\n\n*   **Post-LN (Original):** The first Transformer models applied LN *after* the main operations (attention and the feed-forward network). This can lead to unstable training for very deep models, often requiring a \"learning rate warm-up\" where the model trains very slowly at first.\n*   **Pre-LN (Modern):** Most modern implementations apply LN *before* the main operations. This is generally more stable, allowing models to train without a warm-up period. While Post-LN can sometimes achieve slightly better results, Pre-LN is often preferred for its stability and reliability.\n\n#### 5.2.2 Substitutes of Layer Normalization\n\nResearchers have explored several alternatives to the standard LN.\n\n*   **AdaNorm:** An approach that adapts the normalization based on the input data, which can help prevent overfitting.\n*   **`l2` normalization:** A simpler form of normalization that is more computationally efficient.\n*   **PowerNorm:** A method that improves on some weaknesses of another technique called Batch Normalization (BN) for text data. It uses running averages for statistics to remain stable.\n\n#### 5.2.3 Normalization-free Transformer\n\nIt is also possible to build Transformers without any normalization layers at all.\n\n*   **ReZero:** This technique replaces LN with a single learnable parameter (alpha, `α`) that starts at zero. This parameter controls how much of the previous layer's output is passed through. This simple change has been shown to lead to faster and more stable training.\n\n### 5.3 Position-wise FFN\n\nThe position-wise feed-forward network (FFN) is a key component inside each Transformer block that processes the output of the attention mechanism. This section covers modifications to the FFN.\n\n#### 5.3.1 Activation Function in FFN\n\nThe FFN uses a non-linear \"activation function\" to process information. While the original Transformer used ReLU, other functions have proven more effective.\n\n*   **GELU (Gaussian Error Linear Unit):** Used by default in popular models like BERT and GPT.\n*   **Swish/SiLU:** An activation function that often performs better than ReLU.\n*   **GLU (Gated Linear Unit) variants:** These functions add extra parameters and a gating mechanism, which can improve performance but also increases model size.\n\n#### 5.3.2 Adapting FFN for Larger Capacity\n\nOne way to make a Transformer more powerful is to give it a larger FFN to store more knowledge.\n\n*   **Mixture-of-Experts (MoE):** Instead of having one large FFN, MoE uses many smaller, specialized FFNs called \"experts.\" For each input token, a \"gating network\" decides which one or two experts are best suited to process it. This allows the model to be massive in size (containing many experts) while remaining computationally efficient, since only a fraction of the experts are used for any given input.\n\n#### 5.3.3 Dropping FFN Layers\n\nCounter-intuitively, some research shows that the FFN layers can be completely removed from a Transformer, simplifying the architecture.\n\nTo make this work, the attention mechanism is modified to take on the FFN's role. This is done by adding a set of global \"key-value\" memory slots that all inputs can attend to. This allows the attention layer to perform the same kind of transformations the FFN would have. In decoders, this can significantly reduce the number of parameters and increase speed with little to no loss in performance.",
      "summary": "This section details module-level modifications to the standard Transformer architecture, focusing on three key areas: position representations, layer normalization, and the position-wise feed-forward network (FFN).\n\nTo address the Transformer's inherent lack of order awareness, various positional encoding methods are used. These include the original sinusoidal absolute position embeddings, learnable absolute embeddings, and more effective relative position representations that encode the pairwise distance between tokens. Hybrid methods like Rotary Position Embedding (RoPE) have also proven successful.\n\nLayer Normalization (LN) is crucial for stabilizing training. Its placement is a key design choice: \"pre-LN\" (applying LN before the residual connection) is generally more stable and avoids the need for learning rate warm-up compared to the original \"post-LN\" configuration. Alternatives like ReZero replace LN with a single learnable parameter to simplify the architecture and improve convergence.\n\nModifications to the FFN layer include replacing the standard ReLU activation with functions like GELU or Swish. To increase model capacity efficiently, FFNs can be swapped for Mixture-of-Experts (MoE) layers, which route tokens to different \"expert\" sub-networks. In some cases, FFN layers can be simplified or even removed entirely to improve performance and efficiency.",
      "subsections": [
        {
          "level": 2,
          "section_title": "Position Representations",
          "start_page": 20,
          "end_page": 22,
          "rewritten_content": null,
          "summary": null,
          "subsections": [
            {
              "level": 3,
              "section_title": "Absolute Position Representations",
              "start_page": 20,
              "end_page": 20,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            },
            {
              "level": 3,
              "section_title": "Relative Position Representations",
              "start_page": 21,
              "end_page": 21,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            },
            {
              "level": 3,
              "section_title": "Other Representations",
              "start_page": 21,
              "end_page": 21,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            },
            {
              "level": 3,
              "section_title": "Position Representations without Explicit Encoding",
              "start_page": 22,
              "end_page": 22,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            },
            {
              "level": 3,
              "section_title": "Position Representation on Transformer Decoders",
              "start_page": 22,
              "end_page": 22,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            }
          ]
        },
        {
          "level": 2,
          "section_title": "Layer Normalization",
          "start_page": 23,
          "end_page": 24,
          "rewritten_content": null,
          "summary": null,
          "subsections": [
            {
              "level": 3,
              "section_title": "Placement of Layer Normalization",
              "start_page": 23,
              "end_page": 23,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            },
            {
              "level": 3,
              "section_title": "Substitutes of Layer Normalization",
              "start_page": 24,
              "end_page": 24,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            },
            {
              "level": 3,
              "section_title": "Normalization-free Transformer",
              "start_page": 24,
              "end_page": 24,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            }
          ]
        },
        {
          "level": 2,
          "section_title": "Position-wise FFN",
          "start_page": 25,
          "end_page": 26,
          "rewritten_content": null,
          "summary": null,
          "subsections": [
            {
              "level": 3,
              "section_title": "Activation Function in FFN",
              "start_page": 25,
              "end_page": 25,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            },
            {
              "level": 3,
              "section_title": "Adapting FFN for Larger Capacity",
              "start_page": 25,
              "end_page": 25,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            },
            {
              "level": 3,
              "section_title": "Dropping FFN Layers",
              "start_page": 26,
              "end_page": 26,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            }
          ]
        }
      ]
    },
    {
      "level": 1,
      "section_title": "ARCHITECTURE-LEVEL VARIANTS",
      "start_page": 26,
      "end_page": 31,
      "rewritten_content": "## 6. ARCHITECTURE-LEVEL VARIANTS\n\nThis section explores significant modifications to the overall structure of the standard Transformer model, moving beyond simple tweaks to its individual components.\n\n### 6.1 Adapting Transformer to Be Lightweight\n\nSeveral approaches aim to make Transformer models more efficient, reducing their size and computational requirements. This makes them faster and more suitable for devices with limited resources, like mobile phones.\n\n*   **Linformer:** Modifies the self-attention mechanism to be less computationally expensive. It does this by projecting the key and value matrices into a smaller, lower-rank space, which simplifies the attention calculation without losing much performance.\n*   **Transformer-LT:** Uses a two-part architecture. One part uses standard attention to focus on local context (nearby words), while the other part uses efficient convolution and linear layers to capture long-range dependencies.\n*   **Funnel Transformer:** Reduces computation by gradually compressing the sequence of data as it passes through the encoder. It's like a funnel that squeezes the information, processes it, and then expands it again in the decoder.\n*   **DeLighT:** Replaces the standard Transformer block with a more efficient \"expand-and-reduce\" structure. It uses wider but shallower blocks near the input and output, and deeper but narrower blocks in the middle. This results in a much deeper model that is more parameter-efficient than the standard Transformer.\n\n### 6.2 Strengthening Cross-Block Connectivity\n\nIn a standard Transformer, each layer (or block) only receives information from the layer directly before it. This can make it difficult for information to flow through very deep models. The following methods create \"shortcuts\" to improve this flow.\n\n*   **ReaLformer & Predictive Attention Transformer:** These models reuse attention scores from previous layers to help guide the attention calculation in the current layer. This provides a helpful hint from past computations.\n*   **Transparent Attention:** Instead of just using the output from the previous layer, each layer calculates a weighted average of the outputs from *all* preceding layers. This ensures that information from the earliest layers can directly reach the final layers.\n*   **Feedback Transformer:** This modification for the decoder allows each position in the output sequence to attend to a summary of its *own* representations from all previous layers, improving how it builds on its past decisions.\n\n### 6.3 Adaptive Computation Time\n\nA standard Transformer uses the same amount of computation for every piece of data. Adaptive Computation Time (ACT) is a more flexible approach where the model can decide how much effort to spend on a given task.\n\n*   **Hard vs. Easy Examples:** The core idea is that difficult inputs require more processing (feature refinement), while easy inputs can be handled quickly to save resources.\n*   **Universal Transformer (UT):** This model uses a single set of layers that it applies iteratively. It includes a \"halting mechanism\" that decides for each word whether to perform another processing step or stop because the representation is good enough.\n*   **Conditional Computation Transformer (CCT):** This model can decide to skip entire layers if they aren't needed, further improving efficiency.\n\nAnother related technique is **early exiting**, where a model can stop processing through all its layers and produce an output early if it's already confident in its prediction.\n\n*   **DeeBERT & PABEE:** These models check if the output predictions have stabilized over several layers. If the prediction stops changing, they exit early.\n*   **Other Methods:** Researchers have also explored using window-based checks for stability (Li et al.) or a voting system among previous layers (Sun et al.) to decide when to exit.\n\n### 6.4 Transformers with Divide-and-Conquer Strategies\n\nThe computational cost of self-attention grows quadratically with the length of the input sequence, making it very difficult to process long documents. \"Divide-and-conquer\" strategies address this by breaking long sequences into smaller, manageable pieces.\n\n#### 6.4.1 Recurrent Transformers\n\nThis approach processes a long sequence one segment at a time. It maintains a \"memory\" of the segments it has already processed and uses that memory to provide context for the current segment.\n\n*   **Transformer-XL:** A foundational model that caches the hidden states (internal representations) of the previous segment. This cache is then used as an extended context for the current segment, allowing the model to \"see\" further back in the sequence.\n*   **Compressive Transformer:** Extends Transformer-XL by not only caching recent states but also creating a compressed summary of older states. This provides an even longer and more detailed memory.\n*   **Memformer:** Introduces a dedicated external memory that the Transformer can read from and write to, allowing it to explicitly store important information over long periods.\n*   **Other Recurrent Models:** Other works have focused on adding recurrence to pre-trained models like GPT-2 or improving how the memory from previous segments is integrated to create a larger effective context.\n\n#### 6.4.2 Hierarchical Transformers\n\nThis approach organizes the Transformer in a pyramid-like structure. Lower-level Transformers process small chunks of data (like words or sentences), and their outputs are combined and passed to higher-level Transformers that analyze the bigger picture (like paragraphs or entire documents).\n\nThis hierarchical method has two main benefits:\n1.  It allows the model to process very long documents with limited computational resources.\n2.  It can learn richer, more structured representations by analyzing the input at multiple levels of detail.\n\nExamples include using this structure for document-level machine translation, document summarization, and understanding complex document trees. Another use is to create richer features by combining character-level and word-level information, which helps the model handle rare or unknown words. This approach has also been successfully applied to image processing in models like `Image Transformer` and `Transformer in Transformer (TNT)`, which first process small image patches and then combine them to understand the whole image.\n\n### 6.5 Exploring Alternative Architecture\n\nBeyond improving the existing Transformer, some research explores whether the entire architecture can be fundamentally redesigned for better performance.\n\n*   **Sandwich Transformer:** Reorganizes the layers, arguing that attention modules are most effective at the bottom and top layers, while feed-forward networks are more important in the middle layers.\n*   **Mask Attention Network (MAN):** Uses a different kind of attention mechanism based on the relative distance between words, which is often a better fit for language tasks.\n*   **Neural Architecture Search (NAS):** This involves using algorithms to automatically discover new, more effective architectures.\n    *   **Evolved Transformer (ET):** Uses an evolutionary algorithm to search for an optimal design.\n    *   **DARTSformer:** Uses a different search technique to find a better architecture with improved performance.",
      "summary": "This section details architectural variants of the Transformer designed to improve efficiency, performance, and the ability to process long sequences. Key approaches include:\n\n*   **Lightweight Transformers:** Models like Longformer, Lite Transformer, and Funnel Transformer reduce computational and memory costs using techniques such as localized attention, depth-wise convolution, and sequence-length reduction via pooling.\n*   **Strengthened Block Connectivity:** Variants introduce direct connections between non-adjacent blocks to improve gradient flow and information reuse in very deep models.\n*   **Adaptive Computation Time (ACT):** These models dynamically adjust the amount of computation per input, allocating more resources to difficult examples and fewer to easy ones through mechanisms like conditional skipping and early exiting.\n*   **Divide-and-Conquer Strategies:** To manage the quadratic complexity of self-attention on long sequences, these methods break the input into segments. **Recurrent Transformers** (e.g., Transformer-XL) process segments sequentially, caching states to provide context. **Hierarchical Transformers** build multi-level representations by first processing local segments and then aggregating them.\n*   **Alternative Architectures:** Researchers are also exploring more fundamental changes, such as replacing blocks with ODE solvers or using Neural Architecture Search (NAS) to automatically discover novel, performant architectures.",
      "subsections": [
        {
          "level": 2,
          "section_title": "Adapting Transformer to Be Lightweight",
          "start_page": 26,
          "end_page": 26,
          "rewritten_content": null,
          "summary": null,
          "subsections": []
        },
        {
          "level": 2,
          "section_title": "Strengthening Cross-Block Connectivity",
          "start_page": 27,
          "end_page": 27,
          "rewritten_content": null,
          "summary": null,
          "subsections": []
        },
        {
          "level": 2,
          "section_title": "Adaptive Computation Time",
          "start_page": 27,
          "end_page": 28,
          "rewritten_content": null,
          "summary": null,
          "subsections": []
        },
        {
          "level": 2,
          "section_title": "Transformers with Divide-and-Conquer Strategies",
          "start_page": 28,
          "end_page": 30,
          "rewritten_content": null,
          "summary": null,
          "subsections": [
            {
              "level": 3,
              "section_title": "Recurrent Transformers",
              "start_page": 29,
              "end_page": 29,
              "rewritten_content": null,
              "summary": null,
              "subsections": []
            },
            {
              "level": 3,
              "section_title": "Hierarchical Transformers",
              "start_page": 30,
              "end_page": 30,
              "rewritten_content": null,
              "summary": null,
              "subsections": [
                {
                  "level": 4,
                  "section_title": "Hierarchical for low sequence inputs.",
                  "start_page": 30,
                  "end_page": 30,
                  "rewritten_content": null,
                  "summary": null,
                  "subsections": []
                },
                {
                  "level": 4,
                  "section_title": "Hierarchical for richer representations.",
                  "start_page": 30,
                  "end_page": 30,
                  "rewritten_content": null,
                  "summary": null,
                  "subsections": []
                }
              ]
            }
          ]
        },
        {
          "level": 2,
          "section_title": "Exploring Alternative Architecture",
          "start_page": 30,
          "end_page": 31,
          "rewritten_content": null,
          "summary": null,
          "subsections": []
        }
      ]
    },
    {
      "level": 1,
      "section_title": "PRE-TRAINED TRANSFORMERS",
      "start_page": 31,
      "end_page": 31,
      "rewritten_content": "## 7 PRE-TRAINED TRANSFORMERS\n\nA key characteristic of a Transformer is that, unlike other models such as convolutional or recurrent networks, it doesn't make any initial assumptions about the structure of the data. Other models often have a built-in assumption called an \"inductive bias,\" where they expect nearby data points (like adjacent words in a sentence) to be more related.\n\nBecause Transformers lack this bias, they are highly flexible and can identify relationships between data points no matter how far apart they are. However, this flexibility can lead to a problem called \"overfitting\" when there isn't much training data. Overfitting means the model learns the training data too well, including its noise and quirks, and then performs poorly on new, unseen data.\n\nRecent studies show that a powerful way to use Transformers is to \"pre-train\" them on huge amounts of general text data (like the entire internet). From this, the model learns a broad, universal understanding of language that is useful for many specific applications.\n\nThis pre-training is typically done through \"self-supervised\" tasks. A common example is **Masked Language Modeling**, where the model is given a sentence with a missing word and has to predict what the word is. After this initial pre-training, the model can be quickly adapted for a specific job through a process called \"fine-tuning,\" which uses a much smaller, task-specific dataset. This is far more efficient than training a new model from scratch.\n\nHere are the common categories of pre-trained Transformers:\n\n*   **Encoder only:** These models use only the encoder part of the Transformer architecture. They are excellent for tasks that require understanding the context of the text.\n    *   **BERT** is a classic example. It is pre-trained using two tasks: Masked Language Modeling (predicting missing words) and Next Sentence Prediction (determining if two sentences belong together).\n    *   **RoBERTa** is an optimized version of BERT. It improves performance by removing the Next Sentence Prediction task, which was found to sometimes be counterproductive.\n\n*   **Decoder only:** These models use only the decoder part of the Transformer and are typically used for generating text.\n    *   The **Generative Pre-trained Transformer (GPT)** series (including GPT, GPT-2, and GPT-3) is the most well-known example. These models focus on scaling up the size of the decoder to improve performance.\n    *   Large-scale models like GPT-3 have shown an amazing ability to perform tasks with only a few examples provided in the input. This is called \"few-shot performance,\" and it works by giving the model a \"prompt\" that describes the task and provides examples.",
      "summary": "Pre-trained Transformers (PTMs) address the standard Transformer's tendency to overfit on limited data by first training on large corpora. This process uses self-supervised objectives, like masked word prediction, to learn universal language representations, which can then be fine-tuned for specific downstream tasks.\n\nPTMs are categorized by their architecture:\n*   **Encoder-only models**, such as BERT, are primarily used for natural language understanding tasks.\n*   **Decoder-only models**, like the GPT series, are pre-trained for language modeling and have demonstrated impressive few-shot learning performance, particularly at large scales.",
      "subsections": []
    },
    {
      "level": 1,
      "section_title": "APPLICATIONS OF TRANSFORMER",
      "start_page": 32,
      "end_page": 32,
      "rewritten_content": "## 8 APPLICATIONS OF TRANSFORMER\n\nAlthough the Transformer architecture was first created for machine translation, its flexible design has led to its use in many other areas outside of just language, including computer vision and audio processing.\n\n### (1) Natural Language Processing\n\nTransformers are widely used for many Natural Language Processing (NLP) tasks, which involve teaching computers to understand and process human language. Some examples include:\n*   **Machine Translation:** Translating text from one language to another.\n*   **Language Modeling:** Predicting the next word in a sentence.\n*   **Named Entity Recognition:** Identifying key information in text, like names of people or places.\n\nA major reason for the Transformer's success in NLP is the practice of pre-training these models on enormous amounts of text data.\n\n### (2) Computer Vision\n\nTransformers have been adapted for computer vision tasks, which help computers \"see\" and interpret visual information. Examples include:\n*   **Image Classification:** Identifying what is in an image.\n*   **Object Detection:** Locating specific objects within an image.\n*   **Image Generation:** Creating new images from scratch.\n*   **Video Processing:** Analyzing and understanding video content.\n\n### (3) Audio Applications\n\nThe Transformer architecture is also effective for tasks involving audio. These applications include:\n*   **Speech Recognition:** Converting spoken words into text.\n*   **Speech Synthesis:** Generating human-like speech from text.\n*   **Speech Enhancement:** Improving the clarity of audio recordings.\n*   **Music Generation:** Composing new music.\n\n### (4) Multimodal Applications\n\nThanks to its flexible design, the Transformer can also handle \"multimodal\" tasks, which involve processing multiple types of data at once (like text, images, and sound). Some examples are:\n*   **Visual Question Answering:** Answering questions about the content of an image.\n*   **Caption Generation:** Creating a text description for an image.\n*   **Speech-to-Text Translation:** Translating spoken words in one language directly to text in another.\n*   **Text-to-Image Generation:** Creating an image based on a text description.",
      "summary": "Originally designed for machine translation, the Transformer's flexible architecture has enabled its widespread application beyond Natural Language Processing (NLP). It is now extensively used in various domains, including:\n\n*   **Natural Language Processing:** For tasks like machine translation, language modeling, and named entity recognition.\n*   **Computer Vision:** For image classification, object detection, and image generation.\n*   **Audio Applications:** For speech recognition, synthesis, and music generation.\n*   **Multimodal Applications:** For tasks combining modalities, such as visual question answering, caption generation, and text-to-image generation.",
      "subsections": []
    },
    {
      "level": 1,
      "section_title": "CONCLUSION AND FUTURE DIRECTIONS",
      "start_page": 32,
      "end_page": 32,
      "rewritten_content": "## 9. CONCLUSION AND FUTURE DIRECTIONS\n\nThis paper provides a detailed review of different variations of the Transformer model, known as \"X-formers,\" and proposes a new way to categorize them. Researchers have improved the original Transformer model from many angles, including making it more efficient, better at applying its knowledge to new data (generalization), and useful for new applications. These improvements have been achieved by adjusting the model's structure, creating more lightweight designs, and using pre-training techniques.\n\nWhile these advanced X-former models are powerful, challenges remain, especially regarding their efficiency and ability to generalize. Future improvements to Transformer models will likely focus on the following areas:\n\n### (1) Theoretical Analysis\n\nThe Transformer architecture is capable of handling very large training datasets, provided it is built with enough adjustable settings (parameters).\n\n*   **Greater Capacity:** Many studies show that Transformers have a larger \"capacity\" to learn than older models like CNNs (Convolutional Neural Networks) and RNNs (Recurrent Neural Networks). This allows them to process and learn from huge amounts of data.\n*   **Better Performance:** When trained on enough data, a Transformer model usually performs better than a CNN or an RNN.\n*   **Fewer Assumptions:** A simple explanation for this is that the Transformer model is designed with very few built-in assumptions about the data. This makes it highly flexible and allows it to learn patterns directly from the data itself, whereas other models might be more constrained by their initial design.",
      "summary": "This survey provides a comprehensive overview and a new taxonomy for Transformer variants (X-formers), which enhance the original architecture's efficiency, generalization, and application scope. These improvements are achieved through techniques such as incorporating structural priors and designing lightweight architectures. Despite their proven power, challenges remain, particularly in efficiency and generalization. A key future direction is the need for a deeper theoretical analysis to understand why Transformers, with their large capacity and fewer biases, outperform CNNs and RNNs when trained on sufficient data.",
      "subsections": []
    },
    {
      "level": 1,
      "section_title": "REFERENCES",
      "start_page": 33,
      "end_page": 33,
      "rewritten_content": "## REFERENCES\n\n*   [1] Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. (2020).\n    **ETC: Encoding Long and Structured Inputs in Transformers.**\n    Published in *Proceedings of EMNLP*, pages 268–284.\n    [https://doi.org/10.18653/v1/2020.emnlp-main.19](https://doi.org/10.18653/v1/2020.emnlp-main.19)\n\n*   [2] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. (2019).\n    **Character-Level Language Modeling with Deeper Self-Attention.**\n    Published in *Proceedings of AAAI*, pages 3159–3166.\n    [https://doi.org/10.1609/aaai.v33i01.33013159](https://doi.org/10.1609/aaai.v33i01.33013159)\n\n*   [3] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, and Cordelia Schmid. (2021).\n    **ViViT: A Video Vision Transformer.**\n    *arXiv preprint: 2103.15691*.\n\n*   [4] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. (2016).\n    **Layer Normalization.**\n    *arXiv preprint: 1607.06450*.\n\n*   [5] Thomas Bachlechner, Bodhisattwa Prasad Majumder, Huanru Henry Mao, Garrison W. Cottrell, and Julian J. McAuley. (2020).\n    **ReZero is All You Need: Fast Convergence at Large Depth.**\n    *arXiv preprint: 2003.04887*.\n\n*   [6] Alexei Baevski and Michael Auli. (2019).\n    **Adaptive Input Representations for Neural Language Modeling.**\n    Published in *Proceedings of ICLR*.\n    [https://openreview.net/forum?id=ByxZX20qFQ](https://openreview.net/forum?id=ByxZX20qFQ)\n\n*   [7] Ankur Bapna, Naveen Arivazhagan, and Orhan Firat. (2020).\n    **Controlling Computation versus Quality for Neural Sequence Models.**\n    *arXiv preprint: 2002.07106*.\n\n*   [8] Ankur Bapna, Mia Chen, Orhan Firat, Yuan Cao, and Yonghui Wu. (2018).\n    **Training Deeper Neural Machine Translation Models with Transparent Attention.**\n    Published in *Proceedings of EMNLP*, pages 3028–3033.\n    [https://doi.org/10.18653/v1/D18-1338](https://doi.org/10.18653/v1/D18-1338)\n\n*   [9] Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. (2018).\n    **Relational inductive biases, deep learning, and graph networks.**\n    *arXiv preprint: 1806.01261*.\n\n*   [10] Iz Beltagy, Matthew E. Peters, and Arman Cohan. (2020).\n    **Longformer: The Long-Document Transformer.**\n    *arXiv preprint: 2004.05150*.\n\n*   [11] Srinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank J. Reddi, and Sanjiv Kumar. (2020).\n    **Low-Rank Bottleneck in Multi-head Attention Models.**\n    Published in *Proceedings of ICML*, pages 864–873.\n    [http://proceedings.mlr.press/v119/bhojanapalli20a.html](http://proceedings.mlr.press/v119/bhojanapalli20a.html)\n\n*   [12] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan.",
      "summary": "This section lists key academic papers and preprints from 2016 to 2021 that are foundational to the survey on Transformers. The cited works, from major AI conferences (EMNLP, AAAI, ICLR, ICML) and arXiv, cover a range of topics including architectural improvements for handling long inputs (ETC, Longformer), new applications (ViViT for video), and fundamental techniques (Layer Normalization).",
      "subsections": []
    }
  ],
  "tables": [],
  "figures": []
}