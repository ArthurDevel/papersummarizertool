{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Mistral OCR + Annotations (Single Pass, Chunked)\n",
    "\n",
    "This notebook replicates the clean notebook functionality using only the annotations API (with bbox + document annotations) in ≤8-page chunks, then merges results.\n",
    "\n",
    "Outputs:\n",
    "- Per-page markdown\n",
    "- Image bboxes + crops + bbox annotations\n",
    "- Document annotations (language, title, authors, chapter_titles, urls, outline)\n",
    "- Headings from markdown and aligned outline with page/line\n",
    "\n",
    "References: [Basic OCR](https://docs.mistral.ai/capabilities/document_ai/basic_ocr/), [Annotations](https://docs.mistral.ai/capabilities/document_ai/annotations/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Setup\n",
    "Configure client, constants, and convert the PDF to a data URL for simple passing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setup minimal constants and Mistral client.\n",
    "\"\"\"\n",
    "\n",
    "# ### CONSTANTS ###\n",
    "from pathlib import Path\n",
    "NOTEBOOK_NAME: str = \"2025.09.08-test_combined_mistral_ocr_annotations\"\n",
    "PDF_PATH: Path = Path(\"/Users/Focus/Downloads/2212.14024v2.pdf\")\n",
    "MODEL: str = \"mistral-ocr-latest\"\n",
    "DOC_ANNOTATION_MAX_PAGES: int = 8  # per docs\n",
    "\n",
    "# ### DEPENDENCIES ###\n",
    "import os\n",
    "import base64\n",
    "from dotenv import load_dotenv\n",
    "from mistralai import Mistral\n",
    "\n",
    "# ### CLIENT ###\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"MISTRAL_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"MISTRAL_API_KEY not set.\")\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "if not PDF_PATH.exists():\n",
    "    raise FileNotFoundError(f\"PDF not found: {PDF_PATH}\")\n",
    "\n",
    "with open(PDF_PATH, \"rb\") as f:\n",
    "    pdf_bytes = f.read()\n",
    "DOCUMENT_SPEC = {\n",
    "    \"type\": \"document_url\",\n",
    "    \"document_url\": \"data:application/pdf;base64,\" + base64.b64encode(pdf_bytes).decode(\"utf-8\"),\n",
    "}\n",
    "\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Define Schemas\n",
    "Describe bbox annotation and document annotation formats (document includes outline so no extra OCR call is needed).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define simple Pydantic schemas for annotations and outline.\n",
    "\"\"\"\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from mistralai.extra import response_format_from_pydantic_model\n",
    "\n",
    "class BBoxImageAnnotation(BaseModel):\n",
    "    image_type: str = Field(..., description=\"Type of the image (plot/table/diagram/etc)\")\n",
    "    short_description: str = Field(..., description=\"Short description in English\")\n",
    "    summary: str = Field(..., description=\"Longer summary of the image contents\")\n",
    "\n",
    "class OutlineItem(BaseModel):\n",
    "    title: str = Field(..., description=\"Heading text. Only include clear headers dividng the text in sections. These are typically recognized by numbering, and are often in a larger font / in bold.\")\n",
    "    level: int = Field(..., description=\"Heading level 1..6\")\n",
    "\n",
    "class DocumentAnnotation(BaseModel):\n",
    "    language: str = Field(..., description=\"Language of the document\")\n",
    "    title: str | None = Field(None, description=\"Document title if present\")\n",
    "    authors: list[str] = Field(..., description=\"Author names\")\n",
    "    chapter_titles: list[str] = Field(..., description=\"Chapter titles in order\")\n",
    "    urls: list[str] = Field(..., description=\"URLs referenced in the document\")\n",
    "    outline: List[OutlineItem] = Field(default_factory=list, description=\"Document outline (level + title)\")\n",
    "\n",
    "bbox_rf = response_format_from_pydantic_model(BBoxImageAnnotation)\n",
    "doc_rf = response_format_from_pydantic_model(DocumentAnnotation)\n",
    "\n",
    "print(\"Schemas ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Single Chunked OCR + Annotations Pass\n",
    "Request both bbox annotations and document annotations in each batch, merge pages and document fields (one pass).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run chunked OCR+Annotations and produce two variables:\n",
    "- ocr: container with .pages (markdown + images with bbox + crops)\n",
    "- combined: merged doc-level fields (language/title/authors/chapters/urls/outline)\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "import json as _json\n",
    "\n",
    "# Merge helper\n",
    "\n",
    "def merge_doc_annotations(parts: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    result: Dict[str, Any] = {\n",
    "        \"language\": None,\n",
    "        \"title\": None,\n",
    "        \"authors\": [],\n",
    "        \"chapter_titles\": [],\n",
    "        \"urls\": [],\n",
    "        \"outline\": [],\n",
    "    }\n",
    "    seen_authors = set(); seen_chapters = set(); seen_urls = set()\n",
    "    for p in parts:\n",
    "        if not result[\"language\"] and p.get(\"language\"):\n",
    "            result[\"language\"] = p.get(\"language\")\n",
    "        if not result[\"title\"] and p.get(\"title\"):\n",
    "            result[\"title\"] = p.get(\"title\")\n",
    "        for a in p.get(\"authors\", []) or []:\n",
    "            if a not in seen_authors:\n",
    "                seen_authors.add(a); result[\"authors\"].append(a)\n",
    "        for c in p.get(\"chapter_titles\", []) or []:\n",
    "            if c not in seen_chapters:\n",
    "                seen_chapters.add(c); result[\"chapter_titles\"].append(c)\n",
    "        for u in p.get(\"urls\", []) or []:\n",
    "            if u not in seen_urls:\n",
    "                seen_urls.add(u); result[\"urls\"].append(u)\n",
    "        for o in p.get(\"outline\", []) or []:\n",
    "            result[\"outline\"].append(o)\n",
    "    return result\n",
    "\n",
    "collected_pages = []\n",
    "annotation_parts: List[Dict[str, Any]] = []\n",
    "\n",
    "# First batch\n",
    "batch = list(range(DOC_ANNOTATION_MAX_PAGES))\n",
    "resp = client.ocr.process(\n",
    "    model=MODEL,\n",
    "    document=DOCUMENT_SPEC,\n",
    "    pages=batch,\n",
    "    bbox_annotation_format=bbox_rf,\n",
    "    document_annotation_format=doc_rf,\n",
    "    include_image_base64=True,\n",
    ")\n",
    "if not getattr(resp, \"pages\", None):\n",
    "    raise RuntimeError(\"First OCR batch returned no pages.\")\n",
    "collected_pages.extend(resp.pages)\n",
    "raw = resp.document_annotation\n",
    "annotation_parts.append(_json.loads(raw) if isinstance(raw, str) else (raw.model_dump() if hasattr(raw, \"model_dump\") else raw))\n",
    "\n",
    "# Subsequent batches\n",
    "start = DOC_ANNOTATION_MAX_PAGES\n",
    "while True:\n",
    "    batch = list(range(start, start + DOC_ANNOTATION_MAX_PAGES))\n",
    "    resp = client.ocr.process(\n",
    "        model=MODEL,\n",
    "        document=DOCUMENT_SPEC,\n",
    "        pages=batch,\n",
    "        bbox_annotation_format=bbox_rf,\n",
    "        document_annotation_format=doc_rf,\n",
    "        include_image_base64=True,\n",
    "    )\n",
    "    pages_batch = getattr(resp, \"pages\", None) or []\n",
    "    if not pages_batch:\n",
    "        break\n",
    "    collected_pages.extend(pages_batch)\n",
    "    raw = resp.document_annotation\n",
    "    annotation_parts.append(_json.loads(raw) if isinstance(raw, str) else (raw.model_dump() if hasattr(raw, \"model_dump\") else raw))\n",
    "    start += DOC_ANNOTATION_MAX_PAGES\n",
    "\n",
    "class _OCR:\n",
    "    def __init__(self, pages):\n",
    "        self.pages = pages\n",
    "\n",
    "ocr = _OCR(collected_pages)\n",
    "combined = merge_doc_annotations(annotation_parts)\n",
    "\n",
    "print(\"Pages (len):\", len(ocr.pages))\n",
    "print(\"Combined keys:\", list(combined.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Per-page markdown and image crops + bbox coordinates\n",
    "Replicates the clean notebook output: prints markdown, bbox coordinates, and displays cropped images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Print per-page markdown and bbox coordinates; display cropped images.\n",
    "\"\"\"\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image as PILImage\n",
    "import io\n",
    "import base64 as _b64\n",
    "\n",
    "print(\"Pages (len):\", len(ocr.pages))\n",
    "for page in ocr.pages:\n",
    "    dims = getattr(page, \"dimensions\", None)\n",
    "    print(f\"\\n## Page {page.index} | dims: {getattr(dims,'width',None)}x{getattr(dims,'height',None)} dpi={getattr(dims,'dpi',None)}\")\n",
    "    print(getattr(page, \"markdown\", \"\") or \"\")\n",
    "\n",
    "    images = getattr(page, \"images\", []) or []\n",
    "    if not images:\n",
    "        print(\"(no image bboxes)\")\n",
    "    for i, img in enumerate(images, start=1):\n",
    "        tlx = img.top_left_x; tly = img.top_left_y\n",
    "        brx = img.bottom_right_x; bry = img.bottom_right_y\n",
    "        w = brx - tlx; h = bry - tly\n",
    "        print(f\"- Image {i}: id={getattr(img,'id',None)} bbox=({tlx},{tly})→({brx},{bry}) size=({w}x{h})\")\n",
    "\n",
    "        data_str = getattr(img, \"image_base64\", None)\n",
    "        if not data_str:\n",
    "            continue\n",
    "        b64 = data_str.split(\",\", 1)[1] if data_str.startswith(\"data:\") else data_str\n",
    "        image_bytes = _b64.b64decode(b64)\n",
    "        pil_img = PILImage.open(io.BytesIO(image_bytes))\n",
    "        display(pil_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Document annotation summary\n",
    "Print language, title, authors, chapter titles, URLs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Print merged document-level fields.\n",
    "\"\"\"\n",
    "\n",
    "import json as _json\n",
    "\n",
    "print(\"language:\", combined.get(\"language\"))\n",
    "print(\"title:\", combined.get(\"title\"))\n",
    "print(\"authors:\")\n",
    "for a in combined.get(\"authors\", []):\n",
    "    print(\" -\", a)\n",
    "print(\"chapter_titles:\")\n",
    "for t in combined.get(\"chapter_titles\", []):\n",
    "    print(\" -\", t)\n",
    "print(\"urls:\")\n",
    "for u in combined.get(\"urls\", []):\n",
    "    print(\" -\", u)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Headings from Markdown (by page)\n",
    "Extract headings (ATX + setext) with page index and line number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parse markdown headings from OCR pages.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "markdown_headings: List[Dict[str, Any]] = []\n",
    "for page in ocr.pages:\n",
    "    lines = (getattr(page, \"markdown\", \"\") or \"\").splitlines()\n",
    "\n",
    "    # ATX (#..######)\n",
    "    for i, line in enumerate(lines, start=1):\n",
    "        m = re.match(r\"^(#{1,6})\\s+(.*)$\", line)\n",
    "        if m:\n",
    "            markdown_headings.append({\n",
    "                \"page_index\": page.index,\n",
    "                \"line\": i,\n",
    "                \"level\": len(m.group(1)),\n",
    "                \"text\": m.group(2).strip(),\n",
    "            })\n",
    "\n",
    "    # Setext (=== or --- underline)\n",
    "    for i in range(2, len(lines) + 1):\n",
    "        underline = lines[i - 1].strip()\n",
    "        if re.match(r\"^={3,}$\", underline):\n",
    "            markdown_headings.append({\n",
    "                \"page_index\": page.index,\n",
    "                \"line\": i - 1,\n",
    "                \"level\": 1,\n",
    "                \"text\": lines[i - 2].strip(),\n",
    "            })\n",
    "        elif re.match(r\"^-{3,}$\", underline):\n",
    "            markdown_headings.append({\n",
    "                \"page_index\": page.index,\n",
    "                \"line\": i - 1,\n",
    "                \"level\": 2,\n",
    "                \"text\": lines[i - 2].strip(),\n",
    "            })\n",
    "\n",
    "print(f\"Found {len(markdown_headings)} markdown headings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Align Outline to Markdown (RapidFuzz) + Print\n",
    "Align combined outline (level + title) to markdown headings, print readable mapping and save JSON.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Align outline with markdown headings using RapidFuzz; print and save.\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "from rapidfuzz import fuzz\n",
    "import json as _json\n",
    "from pathlib import Path as _Path\n",
    "\n",
    "def _norm(text: str) -> str:\n",
    "    return \" \".join((text or \"\").lower().split())\n",
    "\n",
    "ocr_outline = combined.get(\"outline\", []) or []\n",
    "all_md: List[Dict[str, Any]] = list(markdown_headings)\n",
    "\n",
    "aligned: List[Dict[str, Any]] = []\n",
    "THRESHOLD: int = 85\n",
    "\n",
    "for item in ocr_outline:\n",
    "    title = str(item.get(\"title\") or \"\")\n",
    "    level = int(item.get(\"level\"))\n",
    "\n",
    "    if not all_md:\n",
    "        raise RuntimeError(\"No markdown headings to align to.\")\n",
    "\n",
    "    query = _norm(title)\n",
    "    best_score = -1\n",
    "    best_idx = None\n",
    "    for idx, md_h in enumerate(all_md):\n",
    "        score = fuzz.token_set_ratio(query, _norm(md_h[\"text\"]))\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_idx = idx\n",
    "\n",
    "    rec: Dict[str, Any] = {\n",
    "        \"ocr_title\": title,\n",
    "        \"ocr_level\": level,\n",
    "        \"markdown_page_index\": None,\n",
    "        \"markdown_line\": None,\n",
    "        \"markdown_title\": None,\n",
    "        \"score\": int(best_score) if best_score >= 0 else None,\n",
    "    }\n",
    "\n",
    "    if best_idx is not None and best_score >= THRESHOLD:\n",
    "        md_h = all_md[best_idx]\n",
    "        rec[\"markdown_page_index\"] = md_h[\"page_index\"]\n",
    "        rec[\"markdown_line\"] = md_h[\"line\"]\n",
    "        rec[\"markdown_title\"] = md_h[\"text\"]\n",
    "\n",
    "    aligned.append(rec)\n",
    "\n",
    "print(\"\\nAligned outline with markdown page/line:\")\n",
    "for a in aligned:\n",
    "    print(f\"[page {a.get('markdown_page_index')} line {a.get('markdown_line')}] h{a.get('ocr_level')}: {a.get('ocr_title')} (score={a.get('score')})\")\n",
    "\n",
    "# Save outputs with filename prefix\n",
    "out_dir = _Path.cwd() / \"outputs\"\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "headers_index_file = out_dir / f\"{NOTEBOOK_NAME}_headers_index.json\"\n",
    "headers_normalized_file = out_dir / f\"{NOTEBOOK_NAME}_headers_index_normalized.json\"\n",
    "\n",
    "with open(headers_index_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    _json.dump(markdown_headings, f, ensure_ascii=False, indent=2)\n",
    "with open(headers_normalized_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    _json.dump(aligned, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Saved {headers_index_file} and {headers_normalized_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
