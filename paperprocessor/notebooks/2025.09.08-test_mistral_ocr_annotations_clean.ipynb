{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral OCR: Clean Annotations + Headings Comparison\n",
    "\n",
    "This notebook runs a clean, readable pipeline:\n",
    "- Basic OCR to get per-page markdown and image bboxes (with crops)\n",
    "- Annotations to extract language, title, authors, chapter titles, URLs\n",
    "- Headings extracted two ways for comparison:\n",
    "  - From page markdown (ATX and setext syntax)\n",
    "  - From an OCR-inferred outline (model infers heading levels and pages)\n",
    "\n",
    "References: [Basic OCR](https://docs.mistral.ai/capabilities/document_ai/basic_ocr/), [Annotations](https://docs.mistral.ai/capabilities/document_ai/annotations/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Setup\n",
    "Configures the client, paths, and constants used across the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setup the Mistral client and define basic constants.\n",
    "- Keep code simple and explicit; raise if required variables are missing.\n",
    "\"\"\"\n",
    "\n",
    "# ### CONSTANTS ###\n",
    "from pathlib import Path\n",
    "NOTEBOOK_NAME: str = \"2025.09.08-test_mistral_ocr_annotations_clean\"\n",
    "PDF_PATH: Path = Path(\"/Users/Focus/Downloads/2212.14024v2.pdf\")\n",
    "MODEL: str = \"mistral-ocr-latest\"\n",
    "PAGES: list[int] = list(range(8))  # Document annotation supports up to 8 pages\n",
    "\n",
    "# ### DEPENDENCIES ###\n",
    "import os\n",
    "import base64\n",
    "from dotenv import load_dotenv\n",
    "from mistralai import Mistral\n",
    "\n",
    "# ### SETUP CLIENT ###\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"MISTRAL_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"MISTRAL_API_KEY is not set in environment.\")\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "if not PDF_PATH.exists():\n",
    "    raise FileNotFoundError(f\"PDF not found: {PDF_PATH}\")\n",
    "\n",
    "# Build a data: URL for the PDF (simple and explicit)\n",
    "with open(PDF_PATH, \"rb\") as f:\n",
    "    _pdf_bytes = f.read()\n",
    "DOCUMENT_SPEC = {\n",
    "    \"type\": \"document_url\",\n",
    "    \"document_url\": \"data:application/pdf;base64,\" + base64.b64encode(_pdf_bytes).decode(\"utf-8\"),\n",
    "}\n",
    "\n",
    "print(\"Ready. Model:\", MODEL)\n",
    "print(\"PDF:\", PDF_PATH)\n",
    "print(\"Pages (doc annotation scope):\", PAGES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Basic OCR (markdown + image crops)\n",
    "Runs Basic OCR to get per-page markdown and image bboxes. Also prints page count and shows cropped images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run Basic OCR and display results:\n",
    "- Print per-page markdown\n",
    "- Print bbox coordinates\n",
    "- Display each cropped bbox image\n",
    "- Print number of pages detected\n",
    "\"\"\"\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image as PILImage\n",
    "import io\n",
    "\n",
    "ocr = client.ocr.process(\n",
    "    model=MODEL,\n",
    "    document=DOCUMENT_SPEC,\n",
    "    include_image_base64=True,\n",
    ")\n",
    "\n",
    "print(\"Pages (len):\", len(ocr.pages))\n",
    "for page in ocr.pages:\n",
    "    dims = getattr(page, \"dimensions\", None)\n",
    "    print(f\"\\n## Page {page.index} | dims: {getattr(dims,'width',None)}x{getattr(dims,'height',None)} dpi={getattr(dims,'dpi',None)}\")\n",
    "    print(page.markdown)\n",
    "\n",
    "    images = getattr(page, \"images\", []) or []\n",
    "    if not images:\n",
    "        print(\"(no image bboxes)\")\n",
    "    for i, img in enumerate(images, start=1):\n",
    "        tlx = img.top_left_x\n",
    "        tly = img.top_left_y\n",
    "        brx = img.bottom_right_x\n",
    "        bry = img.bottom_right_y\n",
    "        w = brx - tlx\n",
    "        h = bry - tly\n",
    "        print(f\"- Image {i}: id={getattr(img,'id',None)} bbox=({tlx},{tly})→({brx},{bry}) size=({w}x{h})\")\n",
    "\n",
    "        data_str = img.image_base64\n",
    "        if not data_str:\n",
    "            continue\n",
    "        if data_str.startswith(\"data:\"):\n",
    "            _, b64_data = data_str.split(\",\", 1)\n",
    "        else:\n",
    "            b64_data = data_str\n",
    "        image_bytes = base64.b64decode(b64_data)\n",
    "        pil_img = PILImage.open(io.BytesIO(image_bytes))\n",
    "        display(pil_img)\n",
    "\n",
    "ocr_pages = ocr.pages  # used later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Annotations (language, title, authors, chapters, URLs)\n",
    "Requests document-level annotations and prints them in a readable format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract document-level fields via Document Annotation with batching.\n",
    "Processes the document in 8-page chunks (API limit) and combines results.\n",
    "- Schema: language, title, authors, chapter_titles, urls\n",
    "\"\"\"\n",
    "\n",
    "### IMPORTS ###\n",
    "from pydantic import BaseModel, Field\n",
    "from mistralai.extra import response_format_from_pydantic_model\n",
    "import json as _json\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "### SCHEMA ###\n",
    "class DocumentAnnotation(BaseModel):\n",
    "    language: str = Field(..., description=\"Language of the document\")\n",
    "    title: str | None = Field(None, description=\"Document title if present\")\n",
    "    authors: list[str] = Field(..., description=\"Author names\")\n",
    "    chapter_titles: list[str] = Field(..., description=\"Chapter titles in order\")\n",
    "    urls: list[str] = Field(..., description=\"URLs referenced in the document\")\n",
    "\n",
    "### HELPER FUNCTIONS ###\n",
    "def create_page_batches(total_pages: int, batch_size: int = 8) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Split pages into batches of specified size without overlap.\n",
    "    \n",
    "    Args:\n",
    "        total_pages: Total number of pages in document\n",
    "        batch_size: Maximum pages per batch (API limit is 8)\n",
    "        \n",
    "    Returns:\n",
    "        List of page number lists, e.g. [[0,1,2,3,4,5,6,7], [8,9,10,11,12,13,14]]\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    for start_page in range(0, total_pages, batch_size):\n",
    "        end_page = min(start_page + batch_size, total_pages)\n",
    "        batch_pages = list(range(start_page, end_page))\n",
    "        batches.append(batch_pages)\n",
    "    return batches\n",
    "\n",
    "def process_annotation_batch(client, model: str, document_spec: Dict, pages: List[int], response_format) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a single batch of pages for document annotation.\n",
    "    \n",
    "    Args:\n",
    "        client: Mistral client\n",
    "        model: Model name\n",
    "        document_spec: Document specification\n",
    "        pages: List of page numbers to process\n",
    "        response_format: Pydantic response format\n",
    "        \n",
    "    Returns:\n",
    "        Parsed annotation data as dictionary\n",
    "    \"\"\"\n",
    "    ann = client.ocr.process(\n",
    "        model=model,\n",
    "        document=document_spec,\n",
    "        pages=pages,\n",
    "        document_annotation_format=response_format,\n",
    "        include_image_base64=False,\n",
    "    )\n",
    "\n",
    "    raw = ann.document_annotation\n",
    "    if isinstance(raw, str):\n",
    "        return _json.loads(raw)\n",
    "    elif hasattr(raw, \"model_dump\"):\n",
    "        return raw.model_dump()\n",
    "    elif isinstance(raw, dict):\n",
    "        return raw\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported document_annotation type: {type(raw)}\")\n",
    "\n",
    "def combine_annotations(batch_results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Combine annotation results from multiple batches.\n",
    "    \n",
    "    Args:\n",
    "        batch_results: List of annotation dictionaries from each batch\n",
    "        \n",
    "    Returns:\n",
    "        Combined annotation dictionary\n",
    "    \"\"\"\n",
    "    if not batch_results:\n",
    "        raise ValueError(\"No batch results to combine\")\n",
    "    \n",
    "    # Step 1: Take language and title from first batch (should be consistent)\n",
    "    first_batch = batch_results[0]\n",
    "    combined = {\n",
    "        \"language\": first_batch.get(\"language\"),\n",
    "        \"title\": first_batch.get(\"title\"),\n",
    "    }\n",
    "    \n",
    "    # Step 2: Combine authors from all batches (remove duplicates, preserve order)\n",
    "    all_authors = []\n",
    "    seen_authors = set()\n",
    "    for batch in batch_results:\n",
    "        for author in batch.get(\"authors\", []):\n",
    "            if author.lower() not in seen_authors:\n",
    "                all_authors.append(author)\n",
    "                seen_authors.add(author.lower())\n",
    "    combined[\"authors\"] = all_authors\n",
    "    \n",
    "    # Step 3: Combine chapter titles in order across all batches\n",
    "    all_chapter_titles = []\n",
    "    for batch in batch_results:\n",
    "        all_chapter_titles.extend(batch.get(\"chapter_titles\", []))\n",
    "    combined[\"chapter_titles\"] = all_chapter_titles\n",
    "    \n",
    "    # Step 4: Combine URLs from all batches (remove duplicates)\n",
    "    all_urls = []\n",
    "    seen_urls = set()\n",
    "    for batch in batch_results:\n",
    "        for url in batch.get(\"urls\", []):\n",
    "            if url not in seen_urls:\n",
    "                all_urls.append(url)\n",
    "                seen_urls.add(url)\n",
    "    combined[\"urls\"] = all_urls\n",
    "    \n",
    "    return combined\n",
    "\n",
    "### MAIN PROCESSING ###\n",
    "doc_rf = response_format_from_pydantic_model(DocumentAnnotation)\n",
    "\n",
    "# Step 1: Create page batches based on total pages from OCR\n",
    "total_pages = len(ocr_pages)\n",
    "page_batches = create_page_batches(total_pages, batch_size=8)\n",
    "print(f\"Processing {total_pages} pages in {len(page_batches)} batches:\")\n",
    "for i, batch in enumerate(page_batches):\n",
    "    print(f\"  Batch {i+1}: pages {batch[0]}-{batch[-1]} ({len(batch)} pages)\")\n",
    "\n",
    "# Step 2: Process each batch\n",
    "batch_results = []\n",
    "for i, batch_pages in enumerate(page_batches):\n",
    "    print(f\"Processing batch {i+1}/{len(page_batches)}...\")\n",
    "    batch_result = process_annotation_batch(\n",
    "        client=client,\n",
    "        model=MODEL,\n",
    "        document_spec=DOCUMENT_SPEC,\n",
    "        pages=batch_pages,\n",
    "        response_format=doc_rf\n",
    "    )\n",
    "    batch_results.append(batch_result)\n",
    "\n",
    "# Step 3: Combine results from all batches\n",
    "parsed = combine_annotations(batch_results)\n",
    "\n",
    "# Step 4: Display final results\n",
    "print(\"\\n=== COMBINED DOCUMENT ANNOTATIONS ===\")\n",
    "print(\"language:\", parsed.get(\"language\"))\n",
    "print(\"title:\", parsed.get(\"title\"))\n",
    "print(\"authors:\")\n",
    "for a in parsed.get(\"authors\", []):\n",
    "    print(\" -\", a)\n",
    "print(\"chapter_titles:\")\n",
    "for t in parsed.get(\"chapter_titles\", []):\n",
    "    print(\" -\", t)\n",
    "print(\"urls:\")\n",
    "for u in parsed.get(\"urls\", []):\n",
    "    print(\" -\", u)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Headings from Markdown (by page)\n",
    "Extracts headings from each page’s markdown using ATX and setext rules, and records page index + line number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parse headings from per-page markdown.\n",
    "- ATX: lines starting with 1..6 '#' characters\n",
    "- Setext: lines followed by '===' or '---' underlines\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "markdown_headings: List[Dict[str, Any]] = []\n",
    "for page in ocr_pages:\n",
    "    page_index = page.index\n",
    "    lines = (page.markdown or \"\").splitlines()\n",
    "\n",
    "    # ATX headers\n",
    "    for i, line in enumerate(lines, start=1):\n",
    "        m = re.match(r\"^(#{1,6})\\s+(.*)$\", line)\n",
    "        if m:\n",
    "            level = len(m.group(1))\n",
    "            text = m.group(2).strip()\n",
    "            markdown_headings.append({\n",
    "                \"source\": \"markdown\",\n",
    "                \"page_index\": page_index,\n",
    "                \"line\": i,\n",
    "                \"level\": level,\n",
    "                \"text\": text,\n",
    "            })\n",
    "\n",
    "    # Setext headers\n",
    "    for i in range(2, len(lines) + 1):\n",
    "        underline = lines[i - 1].strip()\n",
    "        if re.match(r\"^={3,}$\", underline):\n",
    "            markdown_headings.append({\n",
    "                \"source\": \"markdown\",\n",
    "                \"page_index\": page_index,\n",
    "                \"line\": i - 1,\n",
    "                \"level\": 1,\n",
    "                \"text\": lines[i - 2].strip(),\n",
    "            })\n",
    "        elif re.match(r\"^-{3,}$\", underline):\n",
    "            markdown_headings.append({\n",
    "                \"source\": \"markdown\",\n",
    "                \"page_index\": page_index,\n",
    "                \"line\": i - 1,\n",
    "                \"level\": 2,\n",
    "                \"text\": lines[i - 2].strip(),\n",
    "            })\n",
    "\n",
    "print(f\"Found {len(markdown_headings)} markdown headings\")\n",
    "for h in markdown_headings:\n",
    "    print(f\"[page {h['page_index']} line {h['line']}] h{h['level']}: {h['text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Headings from OCR-Inferred Outline\n",
    "Requests a structured outline as an annotation, where the model infers heading levels and page indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Request a structured outline as a document annotation with batching.\n",
    "Processes the document in 8-page chunks (API limit) and combines outlines.\n",
    "- The model infers heading level (1..6) only. Page indices are not requested.\n",
    "\"\"\"\n",
    "\n",
    "### IMPORTS ###\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Any\n",
    "from mistralai.extra import response_format_from_pydantic_model\n",
    "import json as _json\n",
    "\n",
    "### SCHEMA ###\n",
    "class OutlineItem(BaseModel):\n",
    "    title: str = Field(..., description=\"Heading text\")\n",
    "    level: int = Field(..., description=\"Heading level 1..6\")\n",
    "\n",
    "class DocumentOutline(BaseModel):\n",
    "    outline: List[OutlineItem] = Field(..., description=\"Document outline\")\n",
    "\n",
    "### HELPER FUNCTIONS ###\n",
    "def process_outline_batch(client, model: str, document_spec: Dict, pages: List[int], response_format) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process a single batch of pages for outline extraction.\n",
    "    \n",
    "    Args:\n",
    "        client: Mistral client\n",
    "        model: Model name\n",
    "        document_spec: Document specification\n",
    "        pages: List of page numbers to process\n",
    "        response_format: Pydantic response format\n",
    "        \n",
    "    Returns:\n",
    "        List of outline items from this batch\n",
    "    \"\"\"\n",
    "    outline_resp = client.ocr.process(\n",
    "        model=model,\n",
    "        document=document_spec,\n",
    "        pages=pages,\n",
    "        document_annotation_format=response_format,\n",
    "        include_image_base64=False,\n",
    "    )\n",
    "\n",
    "    raw_outline = outline_resp.document_annotation\n",
    "    if isinstance(raw_outline, str):\n",
    "        outline_parsed = _json.loads(raw_outline)\n",
    "    elif hasattr(raw_outline, \"model_dump\"):\n",
    "        outline_parsed = raw_outline.model_dump()\n",
    "    elif isinstance(raw_outline, dict):\n",
    "        outline_parsed = raw_outline\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported outline type: {type(raw_outline)}\")\n",
    "    \n",
    "    return outline_parsed.get(\"outline\", [])\n",
    "\n",
    "def combine_outlines(batch_outline_results: List[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Combine outline results from multiple batches.\n",
    "    \n",
    "    Args:\n",
    "        batch_outline_results: List of outline item lists from each batch\n",
    "        \n",
    "    Returns:\n",
    "        Combined list of outline items in order\n",
    "    \"\"\"\n",
    "    if not batch_outline_results:\n",
    "        raise ValueError(\"No batch outline results to combine\")\n",
    "    \n",
    "    # Step 1: Combine all outline items in order\n",
    "    combined_outline = []\n",
    "    for batch_outline in batch_outline_results:\n",
    "        combined_outline.extend(batch_outline)\n",
    "    \n",
    "    return combined_outline\n",
    "\n",
    "### MAIN PROCESSING ###\n",
    "outline_rf = response_format_from_pydantic_model(DocumentOutline)\n",
    "\n",
    "# Step 1: Reuse page batches from document annotation processing\n",
    "# (page_batches was already created in cell 6)\n",
    "print(f\"Processing outline for {len(page_batches)} batches:\")\n",
    "\n",
    "# Step 2: Process each batch for outline\n",
    "batch_outline_results = []\n",
    "for i, batch_pages in enumerate(page_batches):\n",
    "    print(f\"Processing outline batch {i+1}/{len(page_batches)}...\")\n",
    "    batch_outline = process_outline_batch(\n",
    "        client=client,\n",
    "        model=MODEL,\n",
    "        document_spec=DOCUMENT_SPEC,\n",
    "        pages=batch_pages,\n",
    "        response_format=outline_rf\n",
    "    )\n",
    "    batch_outline_results.append(batch_outline)\n",
    "    print(f\"  Found {len(batch_outline)} headings in batch {i+1}\")\n",
    "\n",
    "# Step 3: Combine outline results from all batches\n",
    "ocr_outline = combine_outlines(batch_outline_results)\n",
    "\n",
    "# Step 4: Display final results\n",
    "print(f\"\\n=== COMBINED OCR OUTLINE ===\")\n",
    "print(f\"Found {len(ocr_outline)} outline headings total\")\n",
    "for item in ocr_outline:\n",
    "    print(f\"h{item.get('level')}: {item.get('title')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Compare Headings (Markdown vs OCR-Inferred)\n",
    "Shows both heading lists side-by-side (printed), so you can visually compare consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Print both headings lists for a quick manual comparison.\n",
    "- Markdown headings include page and line\n",
    "- OCR outline includes only level and title (no page)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nMarkdown-derived headings:\")\n",
    "for h in markdown_headings:\n",
    "    print(f\"[page {h['page_index']} line {h['line']}] h{h['level']}: {h['text']}\")\n",
    "\n",
    "print(\"\\nOCR-inferred outline headings:\")\n",
    "for item in ocr_outline:\n",
    "    print(f\"h{item.get('level')}: {item.get('title')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Align OCR Outline to Markdown Lines (RapidFuzz)\n",
    "Matches each OCR-inferred outline heading to the closest markdown heading on the same page to assign an exact markdown line number. This preserves the model’s semantic levels while grounding to precise locations.\n",
    "\n",
    "Note: Requires `rapidfuzz`. If not installed, install it in your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Align OCR outline items to markdown headings using RapidFuzz.\n",
    "Simplified:\n",
    "- Ignore any page indices from the outline (not trustworthy).\n",
    "- Match each OCR outline title against ALL markdown headings in the document.\n",
    "- Assign page and line purely from the best markdown match.\n",
    "Saves `headers_index_normalized.json` with aligned results.\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "from rapidfuzz import fuzz\n",
    "import json as _json\n",
    "from pathlib import Path as _Path\n",
    "\n",
    "def normalize_heading_text(text: str) -> str:\n",
    "    \"\"\"Lowercase and collapse whitespace for robust matching.\"\"\"\n",
    "    return \" \".join((text or \"\").lower().split())\n",
    "\n",
    "# Build a flat list of markdown headings across the whole document\n",
    "all_md: List[Dict[str, Any]] = list(markdown_headings)\n",
    "\n",
    "aligned: List[Dict[str, Any]] = []\n",
    "THRESHOLD: int = 85\n",
    "\n",
    "for item in ocr_outline:\n",
    "    title = str(item.get(\"title\") or \"\")\n",
    "    level = int(item.get(\"level\"))\n",
    "\n",
    "    if not all_md:\n",
    "        aligned.append({\n",
    "            \"page_index\": None,\n",
    "            \"ocr_title\": title,\n",
    "            \"ocr_level\": level,\n",
    "            \"markdown_line\": None,\n",
    "            \"markdown_title\": None,\n",
    "            \"score\": None,\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    query = normalize_heading_text(title)\n",
    "\n",
    "    best_score = -1\n",
    "    best_idx = None\n",
    "    for idx, md_h in enumerate(all_md):\n",
    "        score = fuzz.token_set_ratio(query, normalize_heading_text(md_h[\"text\"]))\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_idx = idx\n",
    "\n",
    "    if best_idx is not None and best_score >= THRESHOLD:\n",
    "        md_h = all_md[best_idx]\n",
    "        aligned.append({\n",
    "            \"page_index\": md_h[\"page_index\"],\n",
    "            \"ocr_title\": title,\n",
    "            \"ocr_level\": level,\n",
    "            \"markdown_line\": md_h[\"line\"],\n",
    "            \"markdown_title\": md_h[\"text\"],\n",
    "            \"score\": int(best_score),\n",
    "        })\n",
    "    else:\n",
    "        aligned.append({\n",
    "            \"page_index\": None,\n",
    "            \"ocr_title\": title,\n",
    "            \"ocr_level\": level,\n",
    "            \"markdown_line\": None,\n",
    "            \"markdown_title\": None,\n",
    "            \"score\": int(best_score) if best_score >= 0 else None,\n",
    "        })\n",
    "\n",
    "print(f\"Aligned {len(aligned)} outline items\")\n",
    "for a in aligned:\n",
    "    print(f\"[page {a['page_index']}] h{a['ocr_level']} → line {a['markdown_line']} (score={a['score']}): {a['ocr_title']}\")\n",
    "\n",
    "# Save outputs with filename prefix\n",
    "out_dir = _Path.cwd() / \"outputs\" \n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "out_path = out_dir / f\"{NOTEBOOK_NAME}_headers_index_normalized.json\"\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    _json.dump(aligned, f, ensure_ascii=False, indent=2)\n",
    "print(\"Saved:\", out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
