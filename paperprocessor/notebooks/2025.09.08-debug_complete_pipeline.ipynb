{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Paper Processing Pipeline Test\n",
    "\n",
    "This notebook walks through each step of the paperprocessor pipeline to verify everything works correctly.\n",
    "\n",
    "## Pipeline Overview\n",
    "1. **PDF to Images**: Convert PDF to page images and create ProcessedDocument\n",
    "2. **OCR to Markdown**: Extract markdown text from each page using Mistral OCR\n",
    "3. **Extract Metadata**: Get title and authors from the document\n",
    "4. **Extract Structure**: Identify headers and structural elements\n",
    "5. **Format Headers**: Convert headers to proper markdown # levels\n",
    "6. **Rewrite Sections**: Clean up and rewrite content sections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Constants\n",
    "\n",
    "First, we'll define all the constants and imports needed for this test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS ###\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import asyncio\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "# Add project root to Python path for imports\n",
    "# When running in Jupyter, we need to go up from notebooks/ to project root\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == \"notebooks\":\n",
    "    project_root = current_dir.parent.parent\n",
    "else:\n",
    "    # Fallback: assume we're in the project root or find it\n",
    "    project_root = current_dir\n",
    "    while project_root.name != \"papersummarizer\" and project_root.parent != project_root:\n",
    "        project_root = project_root.parent\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "print(f\"Added to Python path: {project_root}\")\n",
    "\n",
    "# Import our paperprocessor modules\n",
    "from paperprocessor.models import ProcessedDocument, ProcessedPage\n",
    "from paperprocessor.internals.pdf_to_image import convert_pdf_to_images\n",
    "from paperprocessor.internals.mistral_ocr import extract_markdown_from_pages\n",
    "from paperprocessor.internals.metadata_extractor import extract_metadata\n",
    "from paperprocessor.internals.structure_extractor import extract_structure\n",
    "from paperprocessor.internals.header_formatter import format_headers\n",
    "from paperprocessor.internals.section_rewriter import rewrite_sections\n",
    "\n",
    "### CONSTANTS ###\n",
    "# Path to input PDF file - using the magnetic interactions paper for testing\n",
    "INPUT_PDF_PATH = \"/Users/Focus/Downloads/2212.14024v2.pdf\"\n",
    "\n",
    "# Get the notebook filename for output naming\n",
    "NOTEBOOK_NAME = Path(__file__).stem if \"__file__\" in globals() else \"2025.09.08-test_complete_pipeline\"\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = Path(\"outputs\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Setup logging to see what's happening\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(f\"üìÑ Input PDF: {INPUT_PDF_PATH}\")\n",
    "print(f\"üìÅ Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"üìù Notebook name: {NOTEBOOK_NAME}\")\n",
    "print(f\"‚úÖ Setup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready to Start\n",
    "\n",
    "Let's begin the pipeline test!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ready to start pipeline!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Input PDF\n",
    "\n",
    "First, we'll load the PDF file that we want to process through our pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PDF file\n",
    "print(\"üìÇ Loading PDF file...\")\n",
    "\n",
    "if not os.path.exists(INPUT_PDF_PATH):\n",
    "    raise FileNotFoundError(f\"PDF file not found at: {INPUT_PDF_PATH}\")\n",
    "\n",
    "with open(INPUT_PDF_PATH, \"rb\") as f:\n",
    "    pdf_contents = f.read()\n",
    "\n",
    "pdf_size_mb = len(pdf_contents) / (1024 * 1024)\n",
    "print(f\"‚úÖ PDF loaded successfully!\")\n",
    "print(f\"üìä File size: {pdf_size_mb:.2f} MB\")\n",
    "print(f\"üìä File size: {len(pdf_contents):,} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: PDF to Images and Create Document\n",
    "\n",
    "Convert PDF to images and create the ProcessedDocument object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "\n",
    "# Convert PDF to images\n",
    "images = await convert_pdf_to_images(pdf_contents)\n",
    "print(f\"Converted PDF to {len(images)} page images\")\n",
    "\n",
    "# Create ProcessedDocument\n",
    "pdf_base64 = base64.b64encode(pdf_contents).decode('utf-8')\n",
    "\n",
    "pages = []\n",
    "for i, image in enumerate(images):\n",
    "    page_num = i + 1\n",
    "    \n",
    "    # Convert PIL Image to base64\n",
    "    buffered = io.BytesIO()\n",
    "    image.save(buffered, format=\"PNG\")\n",
    "    img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "    \n",
    "    page = ProcessedPage(page_number=page_num, img_base64=img_base64)\n",
    "    pages.append(page)\n",
    "\n",
    "document = ProcessedDocument(pdf_base64=pdf_base64, pages=pages)\n",
    "print(f\"Created document with {len(document.pages)} pages\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: OCR - Extract Markdown\n",
    "\n",
    "Run OCR on each page to extract markdown text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run OCR on all pages\n",
    "await extract_markdown_from_pages(document)\n",
    "\n",
    "# Show sample OCR output\n",
    "for page in document.pages[:2]:  # First 2 pages\n",
    "    if page.ocr_markdown:\n",
    "        print(f\"\\nPage {page.page_number} OCR (first 200 chars):\")\n",
    "        print(page.ocr_markdown[:200] + \"...\")\n",
    "\n",
    "ocr_count = sum(1 for p in document.pages if p.ocr_markdown)\n",
    "print(f\"\\nOCR completed on {ocr_count}/{len(document.pages)} pages\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract Metadata\n",
    "\n",
    "Extract title and authors from the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metadata\n",
    "await extract_metadata(document)\n",
    "\n",
    "print(f\"Title: {document.title}\")\n",
    "print(f\"Authors: {document.authors}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Extract Structure\n",
    "\n",
    "Find headers and structural elements in the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract structure\n",
    "await extract_structure(document)\n",
    "\n",
    "print(f\"Found {len(document.headers)} headers\")\n",
    "\n",
    "# Show first few headers\n",
    "for i, header in enumerate(document.headers):\n",
    "    print(f\"{i+1}. Level {header.level}: {header.text[:50]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Format Headers\n",
    "\n",
    "Convert headers to proper markdown # levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format headers\n",
    "await format_headers(document)\n",
    "\n",
    "print(\"Headers formatted to markdown levels\")\n",
    "\n",
    "# Show formatted headers\n",
    "for i, header in enumerate(document.headers[:5]):\n",
    "    level_indicator = \"#\" * header.level\n",
    "    print(f\"{i+1}. {level_indicator} {header.text[:50]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Rewrite Sections\n",
    "\n",
    "Final step - rewrite and clean up the content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite sections\n",
    "await rewrite_sections(document)\n",
    "\n",
    "if document.final_markdown:\n",
    "    print(f\"Final markdown generated ({len(document.final_markdown)} chars)\")\n",
    "    print(f\"\\nFirst 300 chars of final output:\")\n",
    "    print(document.final_markdown[:300] + \"...\")\n",
    "    \n",
    "    # Save final output\n",
    "    output_file = OUTPUT_DIR / f\"{NOTEBOOK_NAME}_final_output.md\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(document.final_markdown)\n",
    "    print(f\"\\nSaved final output to: {output_file}\")\n",
    "else:\n",
    "    print(\"No final markdown generated\")\n",
    "\n",
    "print(f\"\\nPipeline complete! Document has:\")\n",
    "print(f\"- Title: {document.title}\")\n",
    "print(f\"- {len(document.pages)} pages\")\n",
    "print(f\"- {len(document.headers)} headers\")\n",
    "print(f\"- Final markdown: {'Yes' if document.final_markdown else 'No'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
